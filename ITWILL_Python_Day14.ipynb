{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 점심시간 문제 : 파이썬과 mySQL 연동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹스크롤링을 해야하는 이유?\n",
    "\n",
    "1. 데이터 분석을 하려면 데이터가 있어야 해서  \n",
    "    - 데이터 수집을 해야하는데 웹에 다양한 데이터들이 있으므로 자유롭게 데이터를 수집할 수 있어야한다.  \n",
    "\n",
    "\n",
    "2. 수집하는 데이터 : text data, image data, 동영상 data, 음악 data\n",
    "\n",
    "    - text data : 감정분석, 형태소 분석\n",
    "    - image data : 이미지 신경망 학습, 자율 주행 자동차 신경망의 학습 데이터, x-ray 사진을 학습해서 질병판단 신경망\n",
    "    - 동영상 data \n",
    "\n",
    "웹사이트를 방문하게 되면 웹사이트에 robbots.txt 파일이 있다.  \n",
    "http://www.naver.com/robot.txt\n",
    "\n",
    "User-agent: * --> 모든 로봇(robot)을 적용합니다.\n",
    "Disallow: / --> 모든 페이지의 색인(indexing)을 금지합니다.\n",
    "Allow : /$ \n",
    "\n",
    "우리가 어떤 웹페이즈를 크롤링 하려면 원칙적으로는 먼저 __robots.txt__를 하여  \n",
    "파일의 내용을 검토한 후 해당 페이지를 크롤링해도 되는지 여부를 먼저 파악을 해야한다.  \n",
    "\n",
    "__만약, 해당 웹페이지의 소유자가 거부를 한 페이지나 내용은 원칙적으로 크롤링을 하면 안된다.__  \n",
    "\n",
    "크게 해당 서버에 __부하를 주지 않는 한도내__에서 합법적으로 데이터를 스크롤링할 수 있도록 코딩을 해야 한다.\n",
    "\n",
    "ecologicalpyramid.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제247. ecologicalpyramid.html 문서에서 모든 html을 파이썬으로 불러오시오~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<body>\n",
      "<div class=\"ecopyramid\">\n",
      "<ul id=\"producers\">\n",
      "<li class=\"producerlist\">\n",
      "<div class=\"name\">plants</div>\n",
      "<div class=\"number\">100000</div>\n",
      "</li>\n",
      "<li class=\"producerlist\">\n",
      "<div class=\"name\">algae</div>\n",
      "<div class=\"number\">100000</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"primaryconsumers\">\n",
      "<li class=\"primaryconsumerlist\">\n",
      "<div class=\"name\">deer</div>\n",
      "<div class=\"number\">1000</div>\n",
      "</li>\n",
      "<li class=\"primaryconsumerlist\">\n",
      "<div class=\"name\">rabbit</div>\n",
      "<div class=\"number\">2000</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"secondaryconsumers\">\n",
      "<li class=\"secondaryconsumerlist\">\n",
      "<div class=\"name\">fox</div>\n",
      "<div class=\"number\">100</div>\n",
      "</li>\n",
      "<li class=\"secondaryconsumerlist\">\n",
      "<div class=\"name\">bear</div>\n",
      "<div class=\"number\">100</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"tertiaryconsumers\">\n",
      "<li class=\"tertiaryconsumerlist\">\n",
      "<div class=\"name\">lion</div>\n",
      "<div class=\"number\">80</div>\n",
      "</li>\n",
      "<li class=\"tertiaryconsumerlist\">\n",
      "<div class=\"name\">tiger</div>\n",
      "<div class=\"number\">50</div>\n",
      "</li>\n",
      "</ul>\n",
      "</div></body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "print (soup)\n",
    "\n",
    "# 확인 : html 파일 열고 -> 오른쪽 클릭 -> 페이지 소스보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "print (soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제248. ecologicalpyramid.html에서 텍스트만 가져오시오~   \n",
    "## (html 코드 말고 텍스트만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plants 100000 algae 100000 deer 1000 rabbit 2000 fox 100 bear 100 lion 80 tiger 50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "print (soup.get_text(\" \", strip = True)) # \" \"(공백) 과 strip = True (자르기)을 이용하자~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제249.   \n",
    "## ecologicalpyramid.html 문서에서 number 클래스에 있는 모든 텍스트를 가져오시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"number\">100000</div>, <div class=\"number\">100000</div>, <div class=\"number\">1000</div>, <div class=\"number\">2000</div>, <div class=\"number\">100</div>, <div class=\"number\">100</div>, <div class=\"number\">80</div>, <div class=\"number\">50</div>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "result = soup.find_all(class_ = \"number\") # 결과는 list형태로 나옴!\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제250. 위의 리스트에 있는 요소들을 for문으로 하나씩 빼내어서   \n",
    "## html코드말고 text만 출력되게 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "1000\n",
      "2000\n",
      "100\n",
      "100\n",
      "80\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "result = soup.find_all(class_ = \"number\") # div_class에서 number를 가져오는 사람\n",
    "\n",
    "for i in result:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제251. ecologicalpyramid.html에서 숫자말고 문자들만 긁어오시오~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plants\n",
      "algae\n",
      "deer\n",
      "rabbit\n",
      "fox\n",
      "bear\n",
      "lion\n",
      "tiger\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "result = soup.find_all(class_ = \"name\")\n",
    "\n",
    "for i in result:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=4&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "\n",
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=5&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "    \n",
    "(URL에서 패턴을 잘 파악해보자 page=4 --> page=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제252.  ebs 의 레이디 버그 시청자 게시판의 html 문서 전체를 스크롤링 하는 함수를 작성하시오! \n",
    "\n",
    "### ebs서버에 직접들어가서 가져오는 법!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    return  result\n",
    "\n",
    "print (ebs_scroll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제253. 위의 html문서를 BeautifulSoup 모듈로 파싱하고 이 html문서에서   \n",
    "## 텍스트만 가져와서 출력하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.get_text(\" \", strip = True)\n",
    "    \n",
    "    return  result2\n",
    "\n",
    "print(ebs_scroll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제254. ebs html 문서 중에서 p 태그에 해당하는 부분을 모두 가져오시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p') # p 태그에 해당하는 부분만 검색하기\n",
    "    \n",
    "    return  result2\n",
    "\n",
    "print(ebs_scroll()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제253. 위의 p 태그 중에 class 가 con에 해당하는 부분만 스크롤링 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "[<p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    다 긁어와 버릴거야\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    왜 아무리 기다려도 않나오는거죠ㅠㅠ\r\n",
      "빨리 나오면 좋겠어요ㅠ\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...\r\n",
      "그냥 다 봤으면 좋겠는데..\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    \"크리스마스_코딩의밤_모이자~~~~~모임장:유*수\"\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    '갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨 \r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.\r\n",
      "레이디버그 \r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    11살 인데도 레이디버그 정말 제미있음.\r\n",
      "우리집 TV에는 레이디버그와 블렛켓이 나옵니다. \r\n",
      "다른 영웅도 나오면 좋을 탠데요.\r\n",
      "여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.\r\n",
      "제발좀 TV에 다른 영웅도 나오게 해 주세요.\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    저는 인도네시아에서 살고 있는 권희영 입니다.\r\n",
      "저는 레이디버그 no.1 fan이예요. 시랑해요\r\n",
      "그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ \r\n",
      "마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...\r\n",
      "돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....\r\n",
      "많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가\r\n",
      "비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면\r\n",
      "레이디버그 굿즈와 블루레이를 출시해달라!!! \r\n",
      "\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>]\n"
     ]
    }
   ],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "        \n",
    "    print(type(result2))\n",
    "    return  result2\n",
    "\n",
    "print(ebs_scroll()) # 아직 <p>와 </p>가 나오는 상황"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제254. 위의 결과에서 html말고 텍스트만 가져오시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "        \n",
    "    for i in result2:\n",
    "        print(i.get_text())\n",
    "    \n",
    "ebs_scroll() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제255. 위의 텍스트를 좀 더 깔끔하고 예쁘게 출력되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다 긁어와 버릴거야\n",
      "왜 아무리 기다려도 않나오는거죠ㅠㅠ\r\n",
      "빨리 나오면 좋겠어요ㅠ\n",
      "레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ\n",
      "레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...\r\n",
      "그냥 다 봤으면 좋겠는데..\n",
      "저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^\n",
      "혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...\n",
      "요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?\n",
      "빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!\n",
      "\"크리스마스_코딩의밤_모이자~~~~~모임장:유*수\"\n",
      "'갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\n",
      "레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨\n",
      "레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^\n",
      "그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.\r\n",
      "레이디버그\n",
      "11살 인데도 레이디버그 정말 제미있음.\r\n",
      "우리집 TV에는 레이디버그와 블렛켓이 나옵니다. \r\n",
      "다른 영웅도 나오면 좋을 탠데요.\r\n",
      "여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.\r\n",
      "제발좀 TV에 다른 영웅도 나오게 해 주세요.\n",
      "미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...\n",
      "제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅\n",
      "레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요\n",
      "저는 인도네시아에서 살고 있는 권희영 입니다.\r\n",
      "저는 레이디버그 no.1 fan이예요. 시랑해요\r\n",
      "그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ \r\n",
      "마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!\n",
      "레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...\n",
      "성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...\r\n",
      "돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....\r\n",
      "많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가\r\n",
      "비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면\r\n",
      "레이디버그 굿즈와 블루레이를 출시해달라!!!\n"
     ]
    }
   ],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "        \n",
    "    for i in result2:\n",
    "        print(i.get_text(\" \", strip = True))\n",
    "    \n",
    "ebs_scroll() # 이 결과를 리스트에 담아야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제256. 위의 게시판의 글들을 하나의 리스트에 하나하나의 요소로 담으시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['다 긁어와 버릴거야', '왜 아무리 기다려도 않나오는거죠ㅠㅠ\\r\\n빨리 나오면 좋겠어요ㅠ', '레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ', '레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...\\r\\n그냥 다 봤으면 좋겠는데..', '저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^', '혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...', '요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?', '빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!', '\"크리스마스_코딩의밤_모이자~~~~~모임장:유*수\"', \"'갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\", '레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨', '레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^', '그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.\\r\\n레이디버그', '11살 인데도 레이디버그 정말 제미있음.\\r\\n우리집 TV에는 레이디버그와 블렛켓이 나옵니다. \\r\\n다른 영웅도 나오면 좋을 탠데요.\\r\\n여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.\\r\\n제발좀 TV에 다른 영웅도 나오게 해 주세요.', '미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...', '제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅', '레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요', '저는 인도네시아에서 살고 있는 권희영 입니다.\\r\\n저는 레이디버그 no.1 fan이예요. 시랑해요\\r\\n그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ \\r\\n마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!', '레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...', '성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...\\r\\n돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....\\r\\n많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가\\r\\n비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면\\r\\n레이디버그 굿즈와 블루레이를 출시해달라!!!']\n"
     ]
    }
   ],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    return params\n",
    "\n",
    "print(ebs_scroll()) #이렇게 하면 리스트로는 담기지만 \\r과 \\n이 나와서 지저분한 상황."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제257. 위의 결과에서 \\r과 \\n을 삭제하고 params 리스트에 담기게 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params2 = []\n",
    "    \n",
    "    for i in params:\n",
    "        params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 [\"\"\\r\"\"\\n\"]\n",
    "        \n",
    "    return params2\n",
    "\n",
    "print(ebs_scroll()) #이렇게 하면 리스트를 깔끔하게 정리가 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제258. 레이디 버그 게시판에서 게시글을 올린 날짜들을 출력하시오!\n",
    "\n",
    "2017.01.20 23:13  \n",
    "2017.01.12 15:59  \n",
    ".   \n",
    ".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019.12.09 16:42', '2019.12.02 22:23', '2019.10.15 20:49', '2019.09.09 20:44', '2019.07.03 14:28', '2019.03.23 20:51', '2019.03.17 19:39', '2018.12.11 19:19', '2018.12.11 14:59', '2018.12.11 12:06', '2018.09.28 14:29', '2018.08.15 17:50', '2018.08.11 16:30', '2018.08.11 16:29', '2018.07.30 15:55', '2018.07.30 15:51', '2018.07.30 15:47', '2018.05.22 22:31', '2018.02.24 20:37', '2018.02.02 00:45']\n"
     ]
    }
   ],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "\n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params2 = []\n",
    "    \n",
    "    for i in params:\n",
    "        params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "        \n",
    "    return params2\n",
    "\n",
    "print(ebs_scroll()) #이렇게 하면 리스트를 깔끔하게 정리가 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제259. (또 다른 점심시간 문제)  \n",
    "## 우리 배웠던 zip을 사용해서 위의 코드를 수정해서 아래와 같이 출력되게 하시오.\n",
    "\n",
    "2019.12.09 16:42 레이디 버그 너무 재미있어요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.12.09 16:42 다 긁어와 버릴거야\n",
      "2019.12.02 22:23 왜 아무리 기다려도 않나오는거죠ㅠㅠ빨리 나오면 좋겠어요ㅠ\n",
      "2019.10.15 20:49 레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ\n",
      "2019.09.09 20:44 레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...그냥 다 봤으면 좋겠는데..\n",
      "2019.07.03 14:28 저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^\n",
      "2019.03.23 20:51 혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...\n",
      "2019.03.17 19:39 요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?\n",
      "2018.12.11 19:19 빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!\n",
      "2018.12.11 14:59 크리스마스_코딩의밤_모이자~~~~~모임장:유*수\n",
      "2018.12.11 12:06 '갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\n",
      "2018.09.28 14:29 레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨\n",
      "2018.08.15 17:50 레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^\n",
      "2018.08.11 16:30 그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.레이디버그\n",
      "2018.08.11 16:29 11살 인데도 레이디버그 정말 제미있음.우리집 TV에는 레이디버그와 블렛켓이 나옵니다. 다른 영웅도 나오면 좋을 탠데요.여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.제발좀 TV에 다른 영웅도 나오게 해 주세요.\n",
      "2018.07.30 15:55 미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...\n",
      "2018.07.30 15:51 제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅\n",
      "2018.07.30 15:47 레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요\n",
      "2018.05.22 22:31 저는 인도네시아에서 살고 있는 권희영 입니다.저는 레이디버그 no.1 fan이예요. 시랑해요그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ 마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!\n",
      "2018.02.24 20:37 레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...\n",
      "2018.02.02 00:45 성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면레이디버그 굿즈와 블루레이를 출시해달라!!!\n"
     ]
    }
   ],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "    result3 = soup.find_all('p', class_=\"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "\n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params2 = []\n",
    "    \n",
    "    for i in params:\n",
    "        params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "        \n",
    "    \n",
    "    \n",
    "    params3 = []\n",
    "    \n",
    "    for i in result3:\n",
    "        params3.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params4 = []\n",
    "    \n",
    "    for i in params3:\n",
    "        params4.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,j in zip(params2,params4):\n",
    "        print(i,j)\n",
    "    \n",
    "ebs_scroll() #이렇게 하면 리스트를 깔끔하게 정리가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두번째 점심문제. MySQL과 파이썬 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec 10 12:07:28 2019\n",
    "\n",
    "@author: wdp\n",
    "\n",
    "이전에 파이썬 프롬프트에서 conda install pymysql을 해야됨.\n",
    "\"\"\"\n",
    "\n",
    "import pymysql\n",
    "\n",
    "# MySQL Connection 연결\n",
    "conn = pymysql.connect(host = 'localhost', user = 'root', password='tiger',db = 'orcl', charset = 'utf8')\n",
    "\n",
    "# Connection으로부터 Cursor생성\n",
    "curs = conn.cursor()\n",
    "\n",
    "# SQL문 실행\n",
    "sql = \"select * from emp\"\n",
    "curs.execute(sql)\n",
    "\n",
    "# 데이터 Fetch\n",
    "rows = curs.fetchall()\n",
    "\n",
    "for i in rows:\n",
    "    print(i)\n",
    "\n",
    "# Connection 닫기\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제260. ebs레이디 버그 게시판 전체의 글들을 전부 스크롤링 하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "\n",
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=2&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "\n",
    "c.page=1 c.page=2 (숫자만 바뀐다.) --> 패턴 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.page = 1 을 c.page = \"+str(i)+\"로 바꾼 후 for문으로 둘러침.\n",
    "\n",
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    for i in range(1,17):\n",
    "        list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=\"+str(i)+\"&hmpMnuId=106\" \n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        result3 = soup.find_all('p', class_=\"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        params = []\n",
    "\n",
    "        for i in result2:\n",
    "            params.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params2 = []\n",
    "\n",
    "        for i in params:\n",
    "            params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "\n",
    "\n",
    "\n",
    "        params3 = []\n",
    "\n",
    "        for i in result3:\n",
    "            params3.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params4 = []\n",
    "\n",
    "        for i in params3:\n",
    "            params4.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''   \n",
    "\n",
    "\n",
    "\n",
    "        for i,j in zip(params2,params4):\n",
    "            print(i,j)\n",
    "    \n",
    "ebs_scroll() #이렇게 하면 리스트를 깔끔하게 정리가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제261. 변수의 내용을 파일로 저장되게 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'iamveryhansome'\n",
    "f = open(\"d:\\\\data\\\\mydata9.txt\", \"w\", encoding = \"UTF-8\")\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제262. 레이디 버그 게시판의 글들을 mytext10.txt로 저장되게 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.page = 1 을 c.page = \"+str(i)+\"로 바꾼 후 for문으로 둘러침.\n",
    "\n",
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    \n",
    "    f = open(\"d:\\\\data\\\\mydata10.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    for i in range(1,17):\n",
    "        list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=\"+str(i)+\"&hmpMnuId=106\" \n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        result3 = soup.find_all('p', class_=\"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        params = []\n",
    "\n",
    "        for i in result2:\n",
    "            params.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params2 = []\n",
    "\n",
    "        for i in params:\n",
    "            params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "\n",
    "\n",
    "\n",
    "        params3 = []\n",
    "\n",
    "        for i in result3:\n",
    "            params3.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params4 = []\n",
    "\n",
    "        for i in params3:\n",
    "            params4.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''   \n",
    "            \n",
    "        for i,j in zip(params2,params4):\n",
    "            f.write(i + j +'\\n')\n",
    "    \n",
    "    f.close() # f.close() 하기 전에 for문으로 f.write를 계속 돌림으로써 이어써지게 된다.(덮어쓰기 아님)\n",
    "\n",
    "ebs_scroll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제264.   \n",
    "## 중앙일보사 홈페이지에서 인공지능으로 검색했을때 나오는 상세기사의 url들을  \n",
    "\n",
    "## 리스트에 담으시오~  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    \n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url=\"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                print(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "\n",
    "chu_scroll2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제265. 위의 상세기사 url을 리스트에 담아서 retrun되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://news.joins.com/article/23636437', 'https://news.joins.com/article/23636341', 'https://news.joins.com/article/23636077', 'https://news.joins.com/article/23636012', 'https://news.joins.com/article/23636011', 'https://news.joins.com/article/23636006', 'https://news.joins.com/article/23636002', 'https://news.joins.com/article/23635999', 'https://news.joins.com/article/23635986', 'https://news.joins.com/article/23635764']\n"
     ]
    }
   ],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url=\"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        param = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                param.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "                \n",
    "    return param\n",
    "\n",
    "print(chu_scroll2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제266. 상세기사 url 한개로 상세기사 본문을 스크롤링하는 함수를  \n",
    "## chu_detail_scroll2()라는 이름으로 생성하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    list_url=\"https://news.joins.com/article/23651473\"\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "\n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "    result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    for i in result: \n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "        \n",
    "        \n",
    "    return params\n",
    "\n",
    "print(chu_detail_scroll2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 재귀함수를 이용하여 url전체로 상세기사 본문을 스크롤링하는 함수를 짜보시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url= \"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")  # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    list_url= chu_scroll2() # 재귀함수로 불러오기!\n",
    "    params = []\n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "    \n",
    "        for i in result: \n",
    "            params.append(i.get_text(\" \", strip = True))\n",
    "        \n",
    "        \n",
    "    return params\n",
    "\n",
    "print(chu_detail_scroll2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제267. 위에서 리턴한 리스트를 my_text15.txt로 저장되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url= \"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "    params = []\n",
    "    list_url= chu_scroll2() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\mytext14.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            params.append(i.get_text(\"\", strip = True))\n",
    "            \n",
    "    f.write(str(params)) \n",
    "               \n",
    "    f.close()\n",
    "    return params\n",
    "     \n",
    "\n",
    "    \n",
    "print(chu_detail_scroll2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url= \"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "\n",
    "    list_url= chu_scroll2() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\mytext15.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            \n",
    "            f.write(str(i.get_text(\"\", strip = True)) + '\\n\\n') \n",
    "               \n",
    "    f.close()\n",
    "     \n",
    "\n",
    "    \n",
    "chu_detail_scroll2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제268.   \n",
    "## 위의 코드를 조금 수정해서 동아일보사에서 딥러닝으로 기사 검색했을때  \n",
    "## 나오는 상세기사들을 수집하는 함수를 2개 생성하시오~  \n",
    "\n",
    "1. donga_scroll() : 상세기사 url들을 리스트에 담는 함수\n",
    "2. donga_detail_scroll() : 상세기사 url을 가지고 기사를 검색하여 기사들을 donga_deep_learing.txt에 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.donga.com/news/article/all/20191008/97780414/1', 'http://www.donga.com/news/article/all/20191005/97741802/1', 'http://www.donga.com/news/article/all/20191001/97671000/1', 'http://www.donga.com/news/article/all/20190927/97612157/1', 'http://www.donga.com/news/article/all/20190924/97561804/1', 'http://www.donga.com/news/article/all/20190924/97561753/1', 'http://www.donga.com/news/article/all/20190918/97457257/2', 'http://www.donga.com/news/article/all/20190906/97306196/1', 'http://www.donga.com/news/article/all/20190906/97299510/1', 'http://www.donga.com/news/article/all/20190905/97283468/2', 'http://www.donga.com/news/article/all/20190901/97218502/1', 'http://www.donga.com/news/article/all/20190831/97201083/1', 'http://www.donga.com/news/article/all/20190830/97186874/1', 'http://www.donga.com/news/article/all/20190829/97172674/1', 'http://www.donga.com/news/article/all/20190828/97153201/1']\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "def donga_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,50,15):\n",
    "        list_url= \"http://www.donga.com/news/search?p=\"+str(i)+\"&query=%EB%94%A5%EB%9F%AC%EB%8B%9D&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "\n",
    "        \n",
    "        params = []\n",
    "        for i in soup.find_all('p', class_=\"tit\"): \n",
    "            for i2 in i.find_all('a') \n",
    "                params.append(i2.get(\"href\")) \n",
    "    return params\n",
    "\n",
    "print(donga_scroll())\n",
    "\n",
    "#2\n",
    "def donga_detail_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "\n",
    "    list_url= donga_scroll() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\donga_deep_learning.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', class_=\"article_txt\") \n",
    "        for i in result: \n",
    "            \n",
    "            f.write(str(i.get_text(\"\", strip = True)) + '\\n\\n') \n",
    "               \n",
    "    f.close()\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "donga_detail_scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "def donga_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,50,15):\n",
    "        list_url= \"http://www.donga.com/news/search?p=\"+str(i)+\"&query=%EB%94%A5%EB%9F%AC%EB%8B%9D&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('p', class_=\"txt\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "print(donga_scroll())\n",
    "\n",
    "def donga_detail_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "    params = []\n",
    "    list_url= donga_scroll() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\donga_deep_learning.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', class_=\"article_txt\") \n",
    "        for i in result: \n",
    "            params.append(i.get_text(\"\", strip = True))\n",
    "            \n",
    "    f.write(str(params)) \n",
    "               \n",
    "    f.close()\n",
    "    return params\n",
    "     \n",
    "\n",
    "    \n",
    "donga_detail_scroll()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
