{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 점심시간 문제 : 파이썬과 mySQL 연동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹스크롤링을 해야하는 이유?\n",
    "\n",
    "1. 데이터 분석을 하려면 데이터가 있어야 해서  \n",
    "    - 데이터 수집을 해야하는데 웹에 다양한 데이터들이 있으므로 자유롭게 데이터를 수집할 수 있어야한다.  \n",
    "\n",
    "\n",
    "2. 수집하는 데이터 : text data, image data, 동영상 data, 음악 data\n",
    "\n",
    "    - text data : 감정분석, 형태소 분석\n",
    "    - image data : 이미지 신경망 학습, 자율 주행 자동차 신경망의 학습 데이터, x-ray 사진을 학습해서 질병판단 신경망\n",
    "    - 동영상 data \n",
    "\n",
    "웹사이트를 방문하게 되면 웹사이트에 robbots.txt 파일이 있다.  \n",
    "http://www.naver.com/robot.txt\n",
    "\n",
    "User-agent: * --> 모든 로봇(robot)을 적용합니다.\n",
    "Disallow: / --> 모든 페이지의 색인(indexing)을 금지합니다.\n",
    "Allow : /$ \n",
    "\n",
    "우리가 어떤 웹페이즈를 크롤링 하려면 원칙적으로는 먼저 __robots.txt__를 하여  \n",
    "파일의 내용을 검토한 후 해당 페이지를 크롤링해도 되는지 여부를 먼저 파악을 해야한다.  \n",
    "\n",
    "__만약, 해당 웹페이지의 소유자가 거부를 한 페이지나 내용은 원칙적으로 크롤링을 하면 안된다.__  \n",
    "\n",
    "크게 해당 서버에 __부하를 주지 않는 한도내__에서 합법적으로 데이터를 스크롤링할 수 있도록 코딩을 해야 한다.\n",
    "\n",
    "ecologicalpyramid.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제247. ecologicalpyramid.html 문서에서 모든 html을 파이썬으로 불러오시오~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<body>\n",
      "<div class=\"ecopyramid\">\n",
      "<ul id=\"producers\">\n",
      "<li class=\"producerlist\">\n",
      "<div class=\"name\">plants</div>\n",
      "<div class=\"number\">100000</div>\n",
      "</li>\n",
      "<li class=\"producerlist\">\n",
      "<div class=\"name\">algae</div>\n",
      "<div class=\"number\">100000</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"primaryconsumers\">\n",
      "<li class=\"primaryconsumerlist\">\n",
      "<div class=\"name\">deer</div>\n",
      "<div class=\"number\">1000</div>\n",
      "</li>\n",
      "<li class=\"primaryconsumerlist\">\n",
      "<div class=\"name\">rabbit</div>\n",
      "<div class=\"number\">2000</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"secondaryconsumers\">\n",
      "<li class=\"secondaryconsumerlist\">\n",
      "<div class=\"name\">fox</div>\n",
      "<div class=\"number\">100</div>\n",
      "</li>\n",
      "<li class=\"secondaryconsumerlist\">\n",
      "<div class=\"name\">bear</div>\n",
      "<div class=\"number\">100</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"tertiaryconsumers\">\n",
      "<li class=\"tertiaryconsumerlist\">\n",
      "<div class=\"name\">lion</div>\n",
      "<div class=\"number\">80</div>\n",
      "</li>\n",
      "<li class=\"tertiaryconsumerlist\">\n",
      "<div class=\"name\">tiger</div>\n",
      "<div class=\"number\">50</div>\n",
      "</li>\n",
      "</ul>\n",
      "</div></body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "print (soup)\n",
    "\n",
    "# 확인 : html 파일 열고 -> 오른쪽 클릭 -> 페이지 소스보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "print (soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제248. ecologicalpyramid.html에서 텍스트만 가져오시오~   \n",
    "## (html 코드 말고 텍스트만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plants 100000 algae 100000 deer 1000 rabbit 2000 fox 100 bear 100 lion 80 tiger 50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "print (soup.get_text(\" \", strip = True)) # \" \"(공백) 과 strip = True (자르기)을 이용하자~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제249.   \n",
    "## ecologicalpyramid.html 문서에서 number 클래스에 있는 모든 텍스트를 가져오시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"number\">100000</div>, <div class=\"number\">100000</div>, <div class=\"number\">1000</div>, <div class=\"number\">2000</div>, <div class=\"number\">100</div>, <div class=\"number\">100</div>, <div class=\"number\">80</div>, <div class=\"number\">50</div>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "result = soup.find_all(class_ = \"number\") # 결과는 list형태로 나옴!\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제250. 위의 리스트에 있는 요소들을 for문으로 하나씩 빼내어서   \n",
    "## html코드말고 text만 출력되게 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "1000\n",
      "2000\n",
      "100\n",
      "100\n",
      "80\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "result = soup.find_all(class_ = \"number\") # div_class에서 number를 가져오는 사람\n",
    "\n",
    "for i in result:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제251. ecologicalpyramid.html에서 숫자말고 문자들만 긁어오시오~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plants\n",
      "algae\n",
      "deer\n",
      "rabbit\n",
      "fox\n",
      "bear\n",
      "lion\n",
      "tiger\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"d:\\\\data\\\\ecologicalpyramid.html\") as a: # with ~as 로 만들면 a.close()를 안해도 된다.\n",
    "    soup = BeautifulSoup(a, \"lxml\") # lxml 또는 html.parser\n",
    "\n",
    "result = soup.find_all(class_ = \"name\")\n",
    "\n",
    "for i in result:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=4&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "\n",
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=5&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "    \n",
    "(URL에서 패턴을 잘 파악해보자 page=4 --> page=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제252.  ebs 의 레이디 버그 시청자 게시판의 html 문서 전체를 스크롤링 하는 함수를 작성하시오! \n",
    "\n",
    "### ebs서버에 직접들어가서 가져오는 법!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    return  result\n",
    "\n",
    "print (ebs_scroll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제253. 위의 html문서를 BeautifulSoup 모듈로 파싱하고 이 html문서에서   \n",
    "## 텍스트만 가져와서 출력하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.get_text(\" \", strip = True)\n",
    "    \n",
    "    return  result2\n",
    "\n",
    "print(ebs_scroll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제254. ebs html 문서 중에서 p 태그에 해당하는 부분을 모두 가져오시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p') # p 태그에 해당하는 부분만 검색하기\n",
    "    \n",
    "    return  result2\n",
    "\n",
    "print(ebs_scroll()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제253. 위의 p 태그 중에 class 가 con에 해당하는 부분만 스크롤링 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "[<p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    다 긁어와 버릴거야\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    왜 아무리 기다려도 않나오는거죠ㅠㅠ\r\n",
      "빨리 나오면 좋겠어요ㅠ\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...\r\n",
      "그냥 다 봤으면 좋겠는데..\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    \"크리스마스_코딩의밤_모이자~~~~~모임장:유*수\"\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    '갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨 \r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.\r\n",
      "레이디버그 \r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    11살 인데도 레이디버그 정말 제미있음.\r\n",
      "우리집 TV에는 레이디버그와 블렛켓이 나옵니다. \r\n",
      "다른 영웅도 나오면 좋을 탠데요.\r\n",
      "여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.\r\n",
      "제발좀 TV에 다른 영웅도 나오게 해 주세요.\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    저는 인도네시아에서 살고 있는 권희영 입니다.\r\n",
      "저는 레이디버그 no.1 fan이예요. 시랑해요\r\n",
      "그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ \r\n",
      "마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>, <p class=\"con\">\r\n",
      "\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t\t  \t    성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...\r\n",
      "돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....\r\n",
      "많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가\r\n",
      "비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면\r\n",
      "레이디버그 굿즈와 블루레이를 출시해달라!!! \r\n",
      "\r\n",
      "\t\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    \r\n",
      "\t\t\t\t\t\t    </p>]\n"
     ]
    }
   ],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "        \n",
    "    print(type(result2))\n",
    "    return  result2\n",
    "\n",
    "print(ebs_scroll()) # 아직 <p>와 </p>가 나오는 상황"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제254. 위의 결과에서 html말고 텍스트만 가져오시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "        \n",
    "    for i in result2:\n",
    "        print(i.get_text())\n",
    "    \n",
    "ebs_scroll() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제255. 위의 텍스트를 좀 더 깔끔하고 예쁘게 출력되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다 긁어와 버릴거야\n",
      "왜 아무리 기다려도 않나오는거죠ㅠㅠ\r\n",
      "빨리 나오면 좋겠어요ㅠ\n",
      "레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ\n",
      "레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...\r\n",
      "그냥 다 봤으면 좋겠는데..\n",
      "저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^\n",
      "혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...\n",
      "요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?\n",
      "빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!\n",
      "\"크리스마스_코딩의밤_모이자~~~~~모임장:유*수\"\n",
      "'갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\n",
      "레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨\n",
      "레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^\n",
      "그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.\r\n",
      "레이디버그\n",
      "11살 인데도 레이디버그 정말 제미있음.\r\n",
      "우리집 TV에는 레이디버그와 블렛켓이 나옵니다. \r\n",
      "다른 영웅도 나오면 좋을 탠데요.\r\n",
      "여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.\r\n",
      "제발좀 TV에 다른 영웅도 나오게 해 주세요.\n",
      "미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...\n",
      "제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅\n",
      "레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요\n",
      "저는 인도네시아에서 살고 있는 권희영 입니다.\r\n",
      "저는 레이디버그 no.1 fan이예요. 시랑해요\r\n",
      "그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ \r\n",
      "마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!\n",
      "레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...\n",
      "성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...\r\n",
      "돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....\r\n",
      "많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가\r\n",
      "비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면\r\n",
      "레이디버그 굿즈와 블루레이를 출시해달라!!!\n"
     ]
    }
   ],
   "source": [
    "import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "def  ebs_scroll():\n",
    "\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "        \n",
    "    for i in result2:\n",
    "        print(i.get_text(\" \", strip = True))\n",
    "    \n",
    "ebs_scroll() # 이 결과를 리스트에 담아야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제256. 위의 게시판의 글들을 하나의 리스트에 하나하나의 요소로 담으시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['다 긁어와 버릴거야', '왜 아무리 기다려도 않나오는거죠ㅠㅠ\\r\\n빨리 나오면 좋겠어요ㅠ', '레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ', '레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...\\r\\n그냥 다 봤으면 좋겠는데..', '저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^', '혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...', '요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?', '빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!', '\"크리스마스_코딩의밤_모이자~~~~~모임장:유*수\"', \"'갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\", '레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨', '레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^', '그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.\\r\\n레이디버그', '11살 인데도 레이디버그 정말 제미있음.\\r\\n우리집 TV에는 레이디버그와 블렛켓이 나옵니다. \\r\\n다른 영웅도 나오면 좋을 탠데요.\\r\\n여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.\\r\\n제발좀 TV에 다른 영웅도 나오게 해 주세요.', '미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...', '제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅', '레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요', '저는 인도네시아에서 살고 있는 권희영 입니다.\\r\\n저는 레이디버그 no.1 fan이예요. 시랑해요\\r\\n그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ \\r\\n마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!', '레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...', '성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...\\r\\n돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....\\r\\n많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가\\r\\n비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면\\r\\n레이디버그 굿즈와 블루레이를 출시해달라!!!']\n"
     ]
    }
   ],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    return params\n",
    "\n",
    "print(ebs_scroll()) #이렇게 하면 리스트로는 담기지만 \\r과 \\n이 나와서 지저분한 상황."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제257. 위의 결과에서 \\r과 \\n을 삭제하고 params 리스트에 담기게 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('p', class_ = \"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params2 = []\n",
    "    \n",
    "    for i in params:\n",
    "        params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 [\"\"\\r\"\"\\n\"]\n",
    "        \n",
    "    return params2\n",
    "\n",
    "print(ebs_scroll()) #이렇게 하면 리스트를 깔끔하게 정리가 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제258. 레이디 버그 게시판에서 게시글을 올린 날짜들을 출력하시오!\n",
    "\n",
    "2017.01.20 23:13  \n",
    "2017.01.12 15:59  \n",
    ".   \n",
    ".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019.12.09 16:42', '2019.12.02 22:23', '2019.10.15 20:49', '2019.09.09 20:44', '2019.07.03 14:28', '2019.03.23 20:51', '2019.03.17 19:39', '2018.12.11 19:19', '2018.12.11 14:59', '2018.12.11 12:06', '2018.09.28 14:29', '2018.08.15 17:50', '2018.08.11 16:30', '2018.08.11 16:29', '2018.07.30 15:55', '2018.07.30 15:51', '2018.07.30 15:47', '2018.05.22 22:31', '2018.02.24 20:37', '2018.02.02 00:45']\n"
     ]
    }
   ],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "\n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params2 = []\n",
    "    \n",
    "    for i in params:\n",
    "        params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "        \n",
    "    return params2\n",
    "\n",
    "print(ebs_scroll()) #이렇게 하면 리스트를 깔끔하게 정리가 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제259. (또 다른 점심시간 문제)  \n",
    "## 우리 배웠던 zip을 사용해서 위의 코드를 수정해서 아래와 같이 출력되게 하시오.\n",
    "\n",
    "2019.12.09 16:42 레이디 버그 너무 재미있어요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.12.09 16:42 다 긁어와 버릴거야\n",
      "2019.12.02 22:23 왜 아무리 기다려도 않나오는거죠ㅠㅠ빨리 나오면 좋겠어요ㅠ\n",
      "2019.10.15 20:49 레이디버그 시즌3도 유료로 다시보기에 넣어주셨으면 좋겠어요 ㅠㅠ 최근 영상이니까 돈을 더 받던가해서...ㅠㅠㅠㅠㅠㅠ\n",
      "2019.09.09 20:44 레이디버그 너무 재밌는데 맛보기 여서 좀 아쉬워요...그냥 다 봤으면 좋겠는데..\n",
      "2019.07.03 14:28 저희반 유연* 선생님이 너무 재밌다고 추천하셨어요 ^_____^\n",
      "2019.03.23 20:51 혹시 이 프로그램 시즌 몇 까지 하나요?? 시즌 10 까지 한다는 말도 있던데...\n",
      "2019.03.17 19:39 요즘 티비에서 하는 게 시즌 3인가요, 시즌 4인가요?\n",
      "2018.12.11 19:19 빨리 레이디버그 뉴 에피소드를  EBS에서 틀어주면 좋겠네요!!!!!!!!!!!\n",
      "2018.12.11 14:59 크리스마스_코딩의밤_모이자~~~~~모임장:유*수\n",
      "2018.12.11 12:06 '갓준구, 오늘부터 본방사수로 수업 빠진다 선언'\n",
      "2018.09.28 14:29 레이디버그 넘모 재밌어요 제 친구 한해빈이 추천해줘서 보았는데 재미떠요!!! 레이디버그 팬중 1인으로써 별 9^99999999999999999999 만큼 주고십어요 레이디버스 사랑해요 레이디버그 짱짱맨\n",
      "2018.08.15 17:50 레이디버그 팬 중 1인입니다!! 제가 요즘 레이디버그 유튜브에서 시즌1부터 시즌2까지 정주행했고 이제 시즌3를 기다리고 있습니다. 저희 집은 안타깝게도 유선 방송이 나오지 않기 때문에 어쩔 수 없이 ebs에서 하는 레이디버그를 기다려야 합니다. 빨리 개편 해주시면 감사하겠습니다!~^^\n",
      "2018.08.11 16:30 그래도 재미있어서 별 5개가 아닌 별 9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999개를 주고 십어요.레이디버그\n",
      "2018.08.11 16:29 11살 인데도 레이디버그 정말 제미있음.우리집 TV에는 레이디버그와 블렛켓이 나옵니다. 다른 영웅도 나오면 좋을 탠데요.여기 다시보깋와 미리보기도 레이디버그와 블랫켓이 나옵니다.제발좀 TV에 다른 영웅도 나오게 해 주세요.\n",
      "2018.07.30 15:55 미라클스톤이 많아서 그런데(사포티편에서 많음을 볼수있음)그 미라클스톤 다 나눠줘서 다 영웅됬으면 좋겠어요.그리고 영웅될 사람은 거의 마스터푸가 근처에 있던대여?음....클로이영웅이라....꿀벌영웅이 로즈면...\n",
      "2018.07.30 15:51 제가 레이디버그를 뒤늦게 보았어요.그리고 극장판도 재밌었고 홈페이지 있는지도 몰랐어요.어떻게 여기오게 됬었냐면 블로그에서 팝업 만들려고 사진 찾다가 복사가 안돼서 EBS라고 돼잇는데에 들어갔더니 홈페이지에 오게 됫어요.엄마께서 전에 로그인 하셨는데 재가입하래서 햇어요.미라큘러스 파이팅\n",
      "2018.07.30 15:47 레이디버그 팬이에욤.나중에 블랫캣과 레이디버그 둘이 정체를 아는 장면이 나오면 좋겟어요.그리고 레나루즈가 계속 나오면 좋겟어요.미라큘러스 많이 만들어주세요.별점 5+1000000000000000000줘도 모자라요\n",
      "2018.05.22 22:31 저는 인도네시아에서 살고 있는 권희영 입니다.저는 레이디버그 no.1 fan이예요. 시랑해요그리고 ladybug 방송 프로그램 별 5개도 모자라요 ㅠㅠㅠㅠㅠㅠㅠㅠ 마음 같아선 별 100개 그제야 만좃 하지 않아요. 더 1000000000000000 만족!!!!!!!!!!\n",
      "2018.02.24 20:37 레이디버그 너무 재밌어요. 다시보기가 무료가 아니라 조금 아쉽네요...\n",
      "2018.02.02 00:45 성인 덕후입니다. 레이디버그 블루레이 좀 내주세요... 굿즈 좀 내주세요...제발...돈이라면 드리겠습니다 탴마머니 plz...부탁이에요...선생님들 제발....많은거 바라지 않습니다...블루레이만이라도...레이디버그 전화를 소장하고 싶은 욕구가비단 저만의 욕구가 아닌 모든 팬들의 마음이라고 생각합니다. 이비에스는 돈을 벌고 싶다면레이디버그 굿즈와 블루레이를 출시해달라!!!\n"
     ]
    }
   ],
   "source": [
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106\" # c.page = 1 -> 1page만 가져옴.\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "    \n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "    \n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "    # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "    \n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "    \n",
    "    result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "    \n",
    "    result3 = soup.find_all('p', class_=\"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "\n",
    "    params = []\n",
    "    \n",
    "    for i in result2:\n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params2 = []\n",
    "    \n",
    "    for i in params:\n",
    "        params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "        \n",
    "    \n",
    "    \n",
    "    params3 = []\n",
    "    \n",
    "    for i in result3:\n",
    "        params3.append(i.get_text(\" \", strip = True))\n",
    "    \n",
    "    params4 = []\n",
    "    \n",
    "    for i in params3:\n",
    "        params4.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,j in zip(params2,params4):\n",
    "        print(i,j)\n",
    "    \n",
    "ebs_scroll() #이렇게 하면 리스트를 깔끔하게 정리가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두번째 점심문제. MySQL과 파이썬 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec 10 12:07:28 2019\n",
    "\n",
    "@author: wdp\n",
    "\n",
    "이전에 파이썬 프롬프트에서 conda install pymysql을 해야됨.\n",
    "\"\"\"\n",
    "\n",
    "import pymysql\n",
    "\n",
    "# MySQL Connection 연결\n",
    "conn = pymysql.connect(host = 'localhost', user = 'root', password='tiger',db = 'orcl', charset = 'utf8')\n",
    "\n",
    "# Connection으로부터 Cursor생성\n",
    "curs = conn.cursor()\n",
    "\n",
    "# SQL문 실행\n",
    "sql = \"select * from emp\"\n",
    "curs.execute(sql)\n",
    "\n",
    "# 데이터 Fetch\n",
    "rows = curs.fetchall()\n",
    "\n",
    "for i in rows:\n",
    "    print(i)\n",
    "\n",
    "# Connection 닫기\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제260. ebs레이디 버그 게시판 전체의 글들을 전부 스크롤링 하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=1&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "\n",
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=2&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\n",
    "\n",
    "c.page=1 c.page=2 (숫자만 바뀐다.) --> 패턴 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.page = 1 을 c.page = \"+str(i)+\"로 바꾼 후 for문으로 둘러침.\n",
    "\n",
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    for i in range(1,17):\n",
    "        list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=\"+str(i)+\"&hmpMnuId=106\" \n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        result3 = soup.find_all('p', class_=\"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        params = []\n",
    "\n",
    "        for i in result2:\n",
    "            params.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params2 = []\n",
    "\n",
    "        for i in params:\n",
    "            params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "\n",
    "\n",
    "\n",
    "        params3 = []\n",
    "\n",
    "        for i in result3:\n",
    "            params3.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params4 = []\n",
    "\n",
    "        for i in params3:\n",
    "            params4.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''   \n",
    "\n",
    "\n",
    "\n",
    "        for i,j in zip(params2,params4):\n",
    "            print(i,j)\n",
    "    \n",
    "ebs_scroll() #이렇게 하면 리스트를 깔끔하게 정리가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제261. 변수의 내용을 파일로 저장되게 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'iamveryhansome'\n",
    "f = open(\"d:\\\\data\\\\mydata9.txt\", \"w\", encoding = \"UTF-8\")\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제262. 레이디 버그 게시판의 글들을 mytext10.txt로 저장되게 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.page = 1 을 c.page = \"+str(i)+\"로 바꾼 후 for문으로 둘러침.\n",
    "\n",
    "def  ebs_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    \n",
    "    f = open(\"d:\\\\data\\\\mydata10.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    for i in range(1,17):\n",
    "        list_url=\"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=\"+str(i)+\"&hmpMnuId=106\" \n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        # 한글이 포합되어져 있는 ebs 게시판의 html코드를 불러오는 코드\n",
    "\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result2 = soup.find_all('span', class_ = \"date\") # span 태그에 해당하는 부분중에서 \"date\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        result3 = soup.find_all('p', class_=\"con\") # p 태그에 해당하는 부분중에서 \"con\"에 해당하는 부분만 검색하기\n",
    "\n",
    "        params = []\n",
    "\n",
    "        for i in result2:\n",
    "            params.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params2 = []\n",
    "\n",
    "        for i in params:\n",
    "            params2.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''\n",
    "\n",
    "\n",
    "\n",
    "        params3 = []\n",
    "\n",
    "        for i in result3:\n",
    "            params3.append(i.get_text(\" \", strip = True))\n",
    "\n",
    "        params4 = []\n",
    "\n",
    "        for i in params3:\n",
    "            params4.append(re.sub('[\"\\r\"\"\\n\"]', '', i)) # 바꿔야되는 조건이 2개이상일때 '[\"\\r\"\\n\"]''   \n",
    "            \n",
    "        for i,j in zip(params2,params4):\n",
    "            f.write(i + j +'\\n')\n",
    "    \n",
    "    f.close() # f.close() 하기 전에 for문으로 f.write를 계속 돌림으로써 이어써지게 된다.(덮어쓰기 아님)\n",
    "\n",
    "ebs_scroll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제264.   \n",
    "## 중앙일보사 홈페이지에서 인공지능으로 검색했을때 나오는 상세기사의 url들을  \n",
    "\n",
    "## 리스트에 담으시오~  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    import re\n",
    "    \n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url=\"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                print(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "\n",
    "chu_scroll2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제265. 위의 상세기사 url을 리스트에 담아서 retrun되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://news.joins.com/article/23636437', 'https://news.joins.com/article/23636341', 'https://news.joins.com/article/23636077', 'https://news.joins.com/article/23636012', 'https://news.joins.com/article/23636011', 'https://news.joins.com/article/23636006', 'https://news.joins.com/article/23636002', 'https://news.joins.com/article/23635999', 'https://news.joins.com/article/23635986', 'https://news.joins.com/article/23635764']\n"
     ]
    }
   ],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url=\"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        param = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                param.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "                \n",
    "    return param\n",
    "\n",
    "print(chu_scroll2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제266. 상세기사 url 한개로 상세기사 본문을 스크롤링하는 함수를  \n",
    "## chu_detail_scroll2()라는 이름으로 생성하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    list_url=\"https://news.joins.com/article/23651473\"\n",
    "\n",
    "    url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "    # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "    result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "\n",
    "    soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "    result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    for i in result: \n",
    "        params.append(i.get_text(\" \", strip = True))\n",
    "        \n",
    "        \n",
    "    return params\n",
    "\n",
    "print(chu_detail_scroll2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 재귀함수를 이용하여 url전체로 상세기사 본문을 스크롤링하는 함수를 짜보시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url= \"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\")  # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    list_url= chu_scroll2() # 재귀함수로 불러오기!\n",
    "    params = []\n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "    \n",
    "        for i in result: \n",
    "            params.append(i.get_text(\" \", strip = True))\n",
    "        \n",
    "        \n",
    "    return params\n",
    "\n",
    "print(chu_detail_scroll2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제267. 위에서 리턴한 리스트를 my_text15.txt로 저장되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url= \"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "    params = []\n",
    "    list_url= chu_scroll2() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\mytext14.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            params.append(i.get_text(\"\", strip = True))\n",
    "            \n",
    "    f.write(str(params)) \n",
    "               \n",
    "    f.close()\n",
    "    return params\n",
    "     \n",
    "\n",
    "    \n",
    "print(chu_detail_scroll2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chu_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        list_url= \"https://news.joins.com/Search/JoongangNews?page=\"+str(i)+\"&Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&SortType=New&SearchCategoryType=JoongangNews\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('h2', class_=\"headline mg\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "def chu_detail_scroll2():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "\n",
    "    list_url= chu_scroll2() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\mytext15.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', id=\"article_body\") # class 이름을 하면 중복되는것이 있을수있기 때문에 id로 검색하는것을 추천.\n",
    "\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            \n",
    "            f.write(str(i.get_text(\"\", strip = True)) + '\\n\\n') \n",
    "               \n",
    "    f.close()\n",
    "     \n",
    "\n",
    "    \n",
    "chu_detail_scroll2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제268.   \n",
    "## 위의 코드를 조금 수정해서 동아일보사에서 딥러닝으로 기사 검색했을때  \n",
    "## 나오는 상세기사들을 수집하는 함수를 2개 생성하시오~  \n",
    "\n",
    "1. donga_scroll() : 상세기사 url들을 리스트에 담는 함수\n",
    "2. donga_detail_scroll() : 상세기사 url을 가지고 기사를 검색하여 기사들을 donga_deep_learing.txt에 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.donga.com/news/article/all/20191008/97780414/1', 'http://www.donga.com/news/article/all/20191005/97741802/1', 'http://www.donga.com/news/article/all/20191001/97671000/1', 'http://www.donga.com/news/article/all/20190927/97612157/1', 'http://www.donga.com/news/article/all/20190924/97561804/1', 'http://www.donga.com/news/article/all/20190924/97561753/1', 'http://www.donga.com/news/article/all/20190918/97457257/2', 'http://www.donga.com/news/article/all/20190906/97306196/1', 'http://www.donga.com/news/article/all/20190906/97299510/1', 'http://www.donga.com/news/article/all/20190905/97283468/2', 'http://www.donga.com/news/article/all/20190901/97218502/1', 'http://www.donga.com/news/article/all/20190831/97201083/1', 'http://www.donga.com/news/article/all/20190830/97186874/1', 'http://www.donga.com/news/article/all/20190829/97172674/1', 'http://www.donga.com/news/article/all/20190828/97153201/1']\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "def donga_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,50,15):\n",
    "        list_url= \"http://www.donga.com/news/search?p=\"+str(i)+\"&query=%EB%94%A5%EB%9F%AC%EB%8B%9D&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "\n",
    "        \n",
    "        params = []\n",
    "        for i in soup.find_all('p', class_=\"tit\"): \n",
    "            for i2 in i.find_all('a') \n",
    "                params.append(i2.get(\"href\")) \n",
    "    return params\n",
    "\n",
    "print(donga_scroll())\n",
    "\n",
    "#2\n",
    "def donga_detail_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "\n",
    "    list_url= donga_scroll() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\donga_deep_learning.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', class_=\"article_txt\") \n",
    "        for i in result: \n",
    "            \n",
    "            f.write(str(i.get_text(\"\", strip = True)) + '\\n\\n') \n",
    "               \n",
    "    f.close()\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "donga_detail_scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.donga.com/news/article/all/20191008/97780414/1', 'http://www.donga.com/news/article/all/20191005/97741802/1', 'http://www.donga.com/news/article/all/20191001/97671000/1', 'http://www.donga.com/news/article/all/20190927/97612157/1', 'http://www.donga.com/news/article/all/20190924/97561804/1', 'http://www.donga.com/news/article/all/20190924/97561753/1', 'http://www.donga.com/news/article/all/20190918/97457257/2', 'http://www.donga.com/news/article/all/20190906/97306196/1', 'http://www.donga.com/news/article/all/20190906/97299510/1', 'http://www.donga.com/news/article/all/20190905/97283468/2', 'http://www.donga.com/news/article/all/20190901/97218502/1', 'http://www.donga.com/news/article/all/20190831/97201083/1', 'http://www.donga.com/news/article/all/20190830/97186874/1', 'http://www.donga.com/news/article/all/20190829/97172674/1', 'http://www.donga.com/news/article/all/20190828/97153201/1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"ETRI, 분당서울대병원과 공동연구, 200회 임상서 75% 정확도국내 연구진이 호흡을 이용해 폐암을 진단할 수 있는 의료용 ‘전자코’를 개발했다.한국전자통신연구원(ETRI)은 날숨을 통해 폐 속 암세포가 만드는 휘발성유기화합물을 감지하는 센서와 폐암 환자를 판별하는 기계학습 알고리즘 기술을 개발, 국제학술지 ‘센서&액추에이트 B(Sensors and Actuators B)’에 발표했다고 8일 밝혔다.논문명은 폐암진단을 위하여 센서시스템을 이용한 날숨에서의 휘발성 유기화합물의 분석이다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });연구진은 사람의 코가 신경세포를 통해 냄새를 맡는 것에 착안, 호흡가스가 들어오면 전자소자를 이용해 사람의 코처럼 냄새를 맡아 전기적 신호로 바꿔 질병유무를 판단·검진하는 기술을 개발했다. 사람 코에서 착안해 기술명을 ‘전자코’라 붙였다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화ETRI의 전자코 시스템은 데스크탑 컴퓨터 크기로 날숨 샘플링부, 금속산화물 화학센서 모듈, 데이터 신호 처리부 등 크게 3부분으로 구성돼 있다.이 기술을 활용하면 사람의 호흡만으로 간단하게 검사가 가능하다. 현재 폐암진단에 주로 사용되는 X선 검사나 CT 검사법은 방사선 노출 위험이 있고 비용이 높아 부담이 크다.검사과정은 검진자의 날숨을 비닐 키트에 담은 뒤 날숨이 찬 비닐에 탄소막대기를 넣어 호흡 중 배출되는 여러 가스 성분들을 막대기에 붙인다.이어 이 막대기를 전자 코 시스템에 넣어 구동하면 내장된 센서를 통해 가스가 붙은 정도에 따라 달라지는 전기저항을 확인한다.날숨의 구성성분 데이터를 알고리즘으로 분석해 환자의 날숨 정보와 비교하면 폐암 유무를 판별하는데 도움을 얻는다는게 연구진의 설명이다.연구진은 분당 서울대병원의 도움으로 폐암 환자 37명과 정상인 48명 날숨을 채취해 200회를 분석한 뒤 데이터베이스화했다.이를 기반으로 기계학습 모델을 동국대와 공동 개발해 적용한 결과, 약 75%의 정확도를 보였다. 또한 임상적 유의성도 확인해 폐암환자 진단 보완재 역할을 충분히 수행할 수 있다는 것도 확인했다.ETRI의 기술은 기존 병원 진단장비에 비해 센서 제작비용이 저렴하고 가격대비 정확도가 높아 폐암 환자의 수술 예후 모니터링은 물론 일반인의 자가 건강 관리에도 활용할 수 있을 것을 기대된다.ETRI는 후속 연구를 통해 환자 정보를 추가로 얻어 빅데이터를 구축하고 딥러닝 알고리즘을 적용, 의료기기 업체에 기술을 이전해 상용화할 계획이다.연구책임자인 ETRI 진단치료기연구실 이대식 박사는 “이 기술이 상용화되면 폐암 진단 관련 의료기기 시장경쟁력 확보는 물론, 정부 건강보험료 지출 비용 절감에 큰 도움이 될 것으로 보인다”고 말했다.공동연구를 수행한 분당서울대병원 흉부외과 전상훈 교수도 “ETRI와의 연구성과를 통해 저렴하면서도 편리하게 폐암발병 여부를 검사할 수 있는 가능성을 기술적으로 확인했다”면서 “정확도 개선과 빅데이터 적용 등을 통해 시스템을 고도화, 국민건강증진에 기여하겠다”고 밝혔다.\\ufeff【대전=뉴시스】창닫기기사를 추천 하셨습니다호흡 이용해 폐암 발견하는 ‘전자코’ 개발…간편·저렴한 폐암 검진 길 열려베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"[허문명의 4차원 토크 ④]자율주행차 전문가 서승우 서울대 전기정보공학부 교수의 “지금 실리콘밸리에서는”[조영철 기자]\\ufeff9월 23일 현대자동차그룹(현대차)이 2조4000억 원을 투자해 미국 앱티브와 손잡고 자율주행 합작법인을 설립할 계획이라고 발표했다. 현대차는 2030년 완전 자율주행차(무인차) 개발을 완료한다는 계획이다.서승우 서울대 전기정보공학부 교수는 우리나라에서 자율주행차를 가장 오래 연구해온 공학자로 통한다. 2015년 제자들과 미국 실리콘밸리에서 자율주행차 스타트업 ‘토르드라이브(ThorDrive)’를 창업해 기술자문을 해주고 있다. 그를 이달 초 서울대 그의 연구실에서 만났다.그동안 현대차에 대한 걱정이 많았는데 이번 승부수가 전환점이 될 수 있을까.“앱티브는 세계 최대 자동차부품 기업 미국 델파이가 기존 자율주행 관련 기업들을 인수합병해 세운 자율주행 전문 기업이다. 50 대 50 합작법인을 세운 것이니 기술 공유와 협력 사업이 동등하게 이뤄져 그동안 현대차에 부족했던 소프트웨어 역량이 강화되리라 믿는다. 충분히 전환점이 될 수 있는 좋은 기회라고 생각한다.”window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });현대차는 그동안 수소차 개발에 주력해왔다.“수소냐 전기냐 하는 것은 동력에 관한 문제라 자율주행 기술과는 다르다. 하지만 기존 전기차 모델로는 장거리 주행이 어렵고 자율주행 레벨이 올라갈수록 전력 소모도 많기 때문에 수소 기술과 결합하면 당연히 시너지 효과가 있을 거라고 본다.”주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화일각에서는 현대차가 자율주행 분야에서 구글, GM에 이어 세계 3위 수준으로 도약할 거라는 전망도 하던데.“워낙에 기존 강자가 많고 탄탄해 3위권은 큰 희망이고, 5~10위권에는 충분히 들어갈 수 있다고 본다.”실리콘밸리 최대 화두는 바이오매년 방학마다 실리콘밸리를 다녀오는 그인지라 실리콘밸리의 흐름을 누구보다 빨리 알 것 같아 지금 그곳에서는 무슨 일들이 벌어지고 있는지 궁금했다.요즘 실리콘밸리의 화두는 뭔가.“여전히 자율주행차를 중심으로 한 AI(인공지능) 빅데이터가 거센 흐름이고, 바이오 분야도 매우 활발하다.”바이오? 우리는 죽을 쑤고 있는데.“우리는 주로 신약 개발 쪽에 집중하지만, 바이오란 게 신약 말고도 질병 진단과 유전자 가위 등등 기술 분야가 무궁무진하다. 요즘엔 반려동물 쪽으로도 확대돼 질병 진단부터 항암제 연구까지 다양한 기술개발이 이뤄지고 있다.”그는 지난 여름방학에도 서울대 학생들과 함께 2주간 UC버클리 일대를 돌면서 이런 흐름을 직접 체험할 수 있었다고 했다.“한국인 졸업생들이 만든 회사 두 곳을 소개하고 싶다. 하나는 유전자 가위 기술을 가진 벤처인데, 손상된 유전자 부위에 물질을 투입해 없애는 기술을 보유하고 있었다. 질병 가능성이 있는 유전자를 미리 제거하는 기술이다. 막 버클리대 박사학위를 받은 30대 후반의 한국인 청년들이 창업한 회사로, 실리콘밸리의 톱 브랜드 벤처캐피털로부터 거액을 투자받아 분위기가 매우 고무돼 있었다. 반려동물 항암치료 연구를 하는 한 벤처는 스탠퍼드대를 졸업한 한국인 청년 두 명이 공동창업자였다. 직원이 10명인데 창업 1년 만에 실리콘밸리 내 동물병원 70여 곳과 계약을 맺어 암에 걸린 반려동물 치료는 물론, 항암제 연구도 하고 있었다. 라식수술에도 유전자가 활용되고 있었다. 지금까지는 의사가 각막 두께만 보는 기계적인 판단만으로 무조건 수술을 했는데, 사실 라식수술 자체가 안 되는 유전자를 가진 사람들이 있다. 라식수술 적합 여부를 보는 유전자 진단 바이오 회사도 있었다.”자율주행차 연구는 어떤가.“미국 시가총액 상위 그룹들이 다 뛰고 있다고 보면 된다. 1등은 구글 웨이모로, 누적 마일 수가 가장 많다. 웨이모의 목표는 장기적으로 자율주행 소프트웨어 기술을 파는 것이다. 차량공유업체 우버와 리프트는 물론, 세계적인 물류업체 페덱스와 유피에스도 열심인데, 그들에게는 자율주행 기술을 이용해 인건비와 운전자의 피로도를 줄이겠다는 현실적인 목표가 있다. 이에 비해 애플, 인텔, 퀄컴 같은 반도체 기업들은 장래 펼쳐질 센서시장을 선점하고자 개발에 매달리고 있다.”센서시장이라고 하면?“자율주행에 들어가는 기본 기술이 센서다. 소프트웨어와 하드웨어를 다 포함해 주변 환경을 인식하는 레이더나 카메라뿐 아니라, 하드웨어로부터 들어오는 처리 알고리즘 소프트웨어 등등 모든 것을 다 포함한다고 보면 된다. 센서가 데이터를 잘 처리해 명확히 인식하면 주변 환경에 대한 종합적인 이해가 빨라지고, 그 결과 정확한 판단과 경로를 선택해 차가 움직이게 된다. 마치 카메라에서 해상도, 거리 측정 등 하드웨어 기술이 발달하면 데이터가 쌓이고, 이를 잘 처리하면 사람 눈(目) 수준까지 되는 것처럼 말이다. 자율주행차 연구는 결국 AI 연구다. AI 연구는 많은 데이터 축적과 이 데이터를 처리할 수 있는 컴퓨터, 컴퓨터와 데이터를 통해 의미 있는 결과를 내는 딥러닝, 즉 처리 알고리즘 작업이 관건인데 이미 5~6년 전 퀀텀 점프를 했고 이에 따라 가장 큰 혜택을 보고 있는 게 자율주행 분야라 할 수 있다. 날씨, 조도, 외부 돌발 상황 등 예전에는 데이터로 처리하지 못했던 것들을 수백여 개 회사가 목숨 걸고 매달리다 보니 발전 수준이 놀랍다. 지금 같은 속도라면 향후 5년 내 미국 자율주행차들이 번잡한 도심까지는 아니어도 일반 주택가를 누비는 수준은 될 것 같다. 3년 전만 해도 이렇게 빠른 성장을 예상하지 못했다. 기술을 어떻게든 상용화해 삶의 질을 바꾸고 싶어 하는 실리콘밸리 개발자들의 에너지와 열정, 노력이 너무 치열하고 뜨거워 ‘무섭다’고 느껴질 정도다.”자율주행 연구에 사활 거는 기업들[조영철 기자]이 대목에서 그는 기술개발에 대한 한국과 미국의 문화 차이를 언급하기도 했다.“미국 개발자들은 목적이 ‘상용화’에 집중돼 있다. 한국 기업과 국가 모두 기술개발을 말하지만 수요자의 니즈(needs)보다 개발 자체에 방점이 찍힌 반면, 미국은 개발 단계에서부터 사용자 입장에서 사용 가치가 있는지, 상용화 가능성은 얼마나 되는지에 집중한다. 제품 디자이너들이 처음부터 개입하는 것도 그런 문화 때문이다. 그러다 보니 개발 속도나 활용이 빠르다.”실리콘밸리의 투자 문화도 궁금하다. 흔히들 실패에 관대하다고 하는데 정말 그런가.“절대 관대하지 않다. 피 같은 돈을 투자해 잃는 것을 좋아할 사람은 아무도 없다. 하지만 실리콘밸리 벤처캐피털리스트들만의 독특한 투자 문화가 있다. 10개 중 1~2개를 대박 쳐 만회하겠다는 전략이다. 투자했다가 손실을 보면 한국은 끝까지 본전 회수에 집착하는데 실리콘밸리 투자자들은 과감하게 포기한다. 어차피 자기 판단에 따라 투자를 결정했고 모두 다 성공할 수 없다는 것을 잘 알기 때문이다. 몇 번 손실이 나더라도 다른 곳에서 대박을 치면 회수 가능하다는 믿음이 있으니까 실패한 사업을 물고 늘어질 필요가 없다. 이는 곧 투자받은 사람에게는 법적 책임이 없다는 얘기다. 이렇다 보니 실패를 경험 삼아 다시 사업 모델을 만든 뒤 다른 투자자를 찾는다. 실패에 관대하다는 것은 달리 말하면 재기의 기회가 있다는 뜻이다. 물론 시장 규모와 생태계 자체가 우리와 달라 바로 비교할 수는 없겠지만 말이다.”요즘 한국 창업자들도 미국시장을 많이 두드리는데….“나도 적극 권한다. 한국의 답답한 현실을 탓할 시간이 있으면 ‘해외로 나가라’고 말하고 싶다. 이제 글로벌시장에서 성공하지 못하면 한국에서도 성공할 수 없다. 중국이든, 미국이든, 일본이든, 유럽이든 눈높이를 글로벌하게 맞추라고 간곡히 호소하고 싶다. 미국만 해도 이민 1~2세대는 세탁소, 슈퍼마켓 등을 운영해 자식들을 가르쳤다. 이 자녀들이 성공해 이제는 주류 사회에 안착한 경우가 많다. 아직도 미국은 한국인의 근면성실함을 인정하고 있다. 하루에 열 몇 시간 일할 수 있는 민족은 이스라엘과 한국뿐이라고 생각한다. 실제로 우리 핏속에는 도전 DNA가 있지 않은가.”그가 잠시 말을 끊더니 “다만 주의할 것이 있다”고 했다.그게 뭔가.“철저한 현지화 전략이다. 미국인들에게 먹혀야 한다는 얘기다. 국내 상당수 회사가 실리콘밸리 창업을 꿈꾸지만 비용 문제에 부딪히거나, 진입 장벽이 높다고 생각해 몇 년 해보고 안 되면 철수하겠다, 즉 ‘간만 보겠다’는 식으로 진출하는 경향이 있다. 그럴 거면 아예 시도조차 하지 말라고 조언하고 싶다. 한국에서 성공했다고 지사를 연다든지, 직원 한두 명 보내겠다는 생각으로 나갔다가는 철저히 망한다. 미국 소비자 눈높이에 맞춰 기술과 상품을 현지화하고 그들이 받아들일 수 있는 수준까지 끌어올리지 않으면 안 된다.”그의 조언은 “우리가 나가는 것도 중요하지만 남들이 들어오게 하는 것에도 공을 들여야 한다”는 말로 이어졌다.“보도가 많이 안 됐지만 최근 미국 인디애나, 앨라배마, 조지아 주지사가 기업 유치를 위해 연달아 한국을 방문했다. 세계에서 가장 잘산다는 나라의 주지사들이 이렇게 발품 팔며 뛰고 있다는 것은 무슨 뜻일까. 한 나라에서 산업과 경제를 부흥시키려면 기업 혼자 힘만으로는 안 된다. 민·관·학이 함께 움직이는 총체적 노력이 필요하다.”그러면서 자신의 경험을 들려줬다.“우리 회사가 처음 미국 진출을 고민하고 있을 때 어디서 어떻게 소식을 들었는지 한 주정부로부터 연락이 왔다. 세제나 부지 같은 경제적 지원뿐 아니라 직원의 자녀교육 등 입체적인 지원책을 펼쳐 보이면서 자기네 주로 오면 정말 잘해주겠다는 거였다. 주정부 내 대학들과 연계해 우리 기술이 뿌리내릴 수 있도록 차세대 학문, 후속 세대 육성 방안까지 제시하는 걸 보고 깜짝 놀랐다.”아무리 밖에서 안이 더 잘 보이는 법이라지만 그의 이야기를 듣다 보니 죽을힘을 다해 뛰고 있는 실리콘밸리의 에너지를 전해 듣는 것만으로도 우리는 지금 뭘 하고 있나 한탄이 절로 나왔다. 내친김에 중국이나 일본의 움직임은 어떤지도 물었다.미국시장, 간보기 전략으로는 백전백패미·중 무역분쟁 여파는 없나.“그동안 중국 돈을 등에 업은 중국계 벤처캐피털리스트들이 신기술을 서치하면서 기술 헌팅을 많이 했는데 무역분쟁 이후 줄었다는 느낌이 확실히 든다. 중국 투자자도 많이 보이지 않는다. 분명히 여파는 있는 것 같은데, 실리콘밸리에서는 전 세계 자본이 움직이니까 큰 임팩트는 없어 보였다.”기자도 지난해 실리콘밸리를 두 차례 방문할 기회가 있었는데 미·중 간 경제적, 인적 네트워크가 워낙 긴밀해 이걸 끊어내는 게 쉽지 않다는 느낌을 받았다.“맞다. 중국은 이미 10년 전부터 놀라울 정도로 기술 습득을 많이 했다. 자율주행 기술만 해도 앞으로 중국이 먼저 발전할 것이라고 본다. 미국은 실리콘밸리와 타 지역의 격차가 크다. 반면, 중국은 신기술에 대한 규제가 허술하고 미국에서 건너간 트레이닝된 연구자들이 전역에서 일하고 있다. 베이징, 상하이, 광저우 등 중국 10대 도시 모두 자율주행버스가 안 다니는 곳이 없다. 심지어 청소차량도 자율주행차다. 중국은 현재 국내에서 자체 발전시킬 수 있는 기술을 이미 다 갖고 있다고 보면 된다. 사생활 보호 차원에서 미국이나 유럽에서는 엄격히 제한하고 있는 안면인식 분야만 해도 수천만 대의 폐쇄회로(CC)TV로 들어오는 정보를 바탕으로 최고 기술력을 갖추고 있다. 차량과 관련된 빅데이터도 중국이 훨씬 많다. 여기에 막대한 시장이 있고 정부까지 나서 총력 지원하고 있다. 인권 민감도도 덜해 미국은 시도조차 못 해보는 실험들이 과감하게 시도되고 있다. 미·중 무역분쟁이 단기적으로는 중국을 흔들 수 있지만 시간이 미국 편이라고 절대 장담할 수 있는 분위기는 아니다. 전 세계 AI 특허와 논문의 절반이 중국에서 나오는 상황이다. 학회장에서 스탠퍼드대, 버클리대 교수들을 만나보면 중국의 기술 발전에 대한 두려움이 크다는 것을 느끼게 된다.”일본 움직임은 어떤가.“한마디로 자기 나라보다 미국에 더 공을 들이는 것 같다고 느낄 정도로 실리콘밸리에 엄청난 투자를 하고 있다. 지사도 많고 연구소도 많다. 도요타, 혼다, 닛산은 미국시장에선 주류다. 이들이 차만 파는 건 아니다. 도요타는 이미 우버를 포함해 실리콘밸리 기업에 4조 원을 투자하고 연구센터도 열었다. 자율주행은 물론, AI 로봇 연구에도 집중하고 있다. 지난해에는 1조 원을 투자해 MIT, 스탠퍼드대, 뉴미시건대와 공동 기술연구도 시작했다. 혼다도 실리콘밸리에 2조5000억 원을 투자했고 닛산 역시 대규모 투자센터를 갖고 있다. 이들이 왜 이렇게 하겠는가. 그만큼 미래를 보고 있다는 거다. 일본을 절대 얕봐선 안 된다. 우리의 경우 오로지 삼성만 실리콘밸리에 연구소를 규모 있게 운영하면서 현지 직원을 뽑고 있는 상황이다.”실리콘밸리에 막대한 돈 쏟아붓는 日[조영철 기자]2009년부터 서울대 지능형자동차IT연구센터장을 맡아 자율주행차 ‘스누버’를 개발, 운용하며 기술과 경험을 쌓은 서 교수는 ‘토르드라이브’를 창업하면서 자율주행 응용 분야로 ‘배달’을 선택했다. 기업이 대부분 승객 운송에 뛰어드는 상황에서 후발 주자로 물건 배송을 택한 것이다.현재 ‘토르드라이브’의 기술력은 어느 수준인가.“여의도를 비롯해 혼잡한 서울 도심을 3년간 6만km 무사고로 주행했다. 이달 중순부터 이마트와 함께 서울지역에서 자율주행 택배를 시작한다. 온라인으로 들어온 주문을 집까지 배달하는 것이다. 핸들에서 손을 떼고 지켜보는 안전운전자는 옆에 있지만 운전에는 관여하지 않는다. 자율주행 기술은 인간이 차량을 모두 제어하는 레벨 제로(0)부터 인간의 개입 없이 AI가 100% 운전하는 5까지 모두 6단계다. 레벨4(특정 지역이나 상황에서도 운전자가 개입하지 않는 것)가 목표인데 현재 이 단계의 80%쯤 올라온 것 같다.”마차를 대신한 기차의 등장이 3차 산업혁명이었듯, 운송 수단의 혁신이야말로 새로운 산업혁명의 동력이라 할 수 있다. 그런 점에서 자율주행차는 4차 산업혁명을 대표하는 자동차와 AI, IT(정보기술)가 결합된 융복합 신산업이다. 서 교수 말대로 세계는 차세대 먹거리를 향해 사활을 걸고 뛰고 있다. 그런데 우리는 ‘조국 블랙홀’ 50일에 국정시계가 멈췄다. 언제까지 이래도 되는 것인가. 인터뷰를 마치고 나오는 발걸음이 한없이 무거웠다.허문명 기자 angelhuh@donga.com《이 기사는주간동아1208호에 실렸습니다》창닫기기사를 추천 하셨습니다세계는 목숨 건 미래 전쟁 중인데 우리는 지금 어디로 가고 있나베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " '4차 산업혁명시대의 본격화로 인해 AI(인공지능)가 사람의 일을 대체하는 사례가 확연히 늘어나고 있다. 목소리를 통해 이야기를 이끄는 역할 역시 이전에는 사람의 일이었으나 앞으로는 음성 합성과 AI 기술의 발달로 인해 탄생한 \\'AI 성우\\'가 상당부분 이 일을 대신하게 될 것으로 예상된다.다만, 문제는 \\'목소리\\'라는 수단의 특징이다. 사람의 목소리는 같은 말을 하더라도 감정에 따라 느낌이 확연하게 달라지며, 뉘양스의 미묘한 변화에 따라 전혀 다른 의미를 품기도 한다. 이러한 아날로그적인 특성을 디지털 기술로 완벽히 재현하는 것이 가능할까?AI 기반 음성 합성기술 스타트업인 \\'네오사피엔스(Neosapience)\\'는 그런 과제를 해결하겠다고 선언했다. 이 회사에서 선보인 \\'타입캐스트(TypeCast)\\' 서비스는 문자를 단순히 음성화 하는 것을 넘어 특정인의 음성을 학습하고 다양한 감정을 표현할 수 있다. 이를 기반으로 배우나 가수, 성우 등의 유명인들이 자신의 목소리를 상품화하는 등의 응용도 가능하다고 강조하고 있다. 9월 27일, 네오사피엔스의 김태수 대표를 만나 그들이 선보인 AI 음성 합성기술의 이모저모에 대해 알아봤다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot(\\'/1249652/Donga_OSV_Desktop_1x1\\', [1, 1], \\'div-gpt-ad-1571163856651-0\\').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display(\\'div-gpt-ad-1571163856651-0\\'); });네오사피엔스 김태수 대표 (출처=IT동아)Q. 본인에 대한 간단한 소개를 부탁한다주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화2007년에 카이스트에서 오디오 머신러닝 관련 연구로 박사학위를 받았으며, 그 후 LG전자에서 3년, 퀄컴에서 7년을 일하며 각종 기술을 개발했다. 특히 퀄컴 근무 시절, 항상 켜져있는 마이크를 이용한 지능형 음성 인식기술을 개발해 2013 MWC(모바일월드콩그레스) 현장에서 첫 선을 보인 바 있다. 이 기술이 아마존의 AI 음성 비서 서비스인 \\'알렉사\\'의 기반이 되었으며, 마이크로소프트 \\'코타나\\'에도 적용됐다. 그 외에도 애플 \\'시리\\'등의 유사 서비스가 등장하는데도 적잖게 영향을 줬다. 네오사피엔스를 설립한 건 2017년 11월의 일이다.Q. 대기업 소속의 안정적인 지위를 버리고 독립하게 된 이유는?사실 2016년 겨울에 갑작스럽게 쓰러져 죽음의 문턱까지 다녀온 적이 있다. 그런 와중에 시간의 유한함을 문득 느꼈다. 내가 떠나면 가족들은 어쩌지? 세상은 나를 어떻게 기억할까? 등의 생각을 하다가 뭔가 좀 크고 의미 있는 일을 해보자, 우리의 삶을 바꿀 수 있는 일을 해보자고 결심했다.Q. 네오사피엔스 타입캐스트 서비스에 어떤 기술을 적용했는가?기존의 음성 합성 기술은 단순히 단어나 음소를 붙여 넣는 방식이라 실제로 결과물을 들어보면 대단히 어색했다. 그래서 딥러닝(인공신경망 기반 기계학습) 기반의 AI 음성 합성 기술을 적용했다. 딥러닝 네트워크를 통해 각종 수치와 수식을 조합, 합성하니 자연스러운 결과물을 얻었다. 감정도 합성 가능하다. 이를테면 화를 내더라도 아주 \\'나긋하게\\' 화를 내는 것이 가능하다. 이런 미묘한 차이점을 정교하고 자연스럽게 표현할 수 있다.Q. 기존 음성 합성 서비스 대비 타입캐스트 서비스의 특징은?대표적인 특징은 톤이 자연스럽다는 것 외에 목소리에 \\'캐릭터\\'를 부여하는 것이 가능하다는 점이다. 이를테면 문재인, 트럼프 대통령 등의 특정인물 목소리를 내는 것이 가능하며, 이미 30여개의 캐릭터가 등록된 상태다. 그리고 성능을 확인한 후 이런 저런 시도를 해봤는데, 처음으로 시도한 것이 외국인의 목소리로 한국어를, 혹은 그 반대로 구사하도록 하는 것이다. 실제로 트럼프 대통령의 목소리로 한국어를, 문재인 대통령의 목소리로 영어를 구사하는 것이 가능하다.트럼프 대통령의 한국어 구사 합성 장면을 담은 네오사피엔스의 유튜브 동영상 (출처=네오사피엔스)Q. 위와 같은 기술적 특성을 이용해 어떤 응용이 가능한가?이를테면 CNN 같은 외국 언론에선 외국인 인터뷰를 영어로 더빙을 하는 경우가 많다. 그리고 어떤 한류스타가 외국 팬을 상대로 외국어 음성 메시지를 전하고자 할 때도 있다. 혹은 외화를 한국어로 더빙하고자 할 때 \\'모건 프리먼\\' 같은 외국 배우가 직접 자신의 목소리로 한국어 연기를 할 수 있다고 생각해보라. 응용 방법은 무궁무진하다.Q. 현재 서비스 상황은 어떠한가?타입캐스트는 지난 6월 중순부터 오픈베타 서비스를 하고 있으며 현재 5,000여명이 쓰고 있다(10월 중 유료화 예정). 특히 유튜버들에게 인기가 높은데, 이들도 자신의 콘텐츠에 넣을 목소리가 필요하기 때문이다. 그리고 중소기업에서 자사의 홍보자료, 안내용 콘텐츠를 만들 때도 종종 이용하고 있다. 이들이 사람을 직접 고용하려면 비용이 많이 들었을 것이다.그리고 대교의 오디오북 사업에도 기술을 공급하고 있다. 대교는 이야기책 등의 문자 콘텐츠를 오디오북으로 만드는 사업을 전개하고 있는데, 성우나 내레이터를 직접 고용하고 녹음 및 편집을 하려면 한 권당 최소 수백만원의 비용과 2주 정도의 시간이 들었다고 한다. 하지만 타입캐스트 적용 이후 작업에 드는 비용과 시간이 대폭 줄어들었다. 현재 대교에서 타입캐스트 기반 오디오북을 실제로 서비스를 하고 있는데, 사람이 직접 녹음한 기존 오디오북과 이용요금이 동일한데도 불구하고 사용자들의 반응은 좋다.Q. 유명인의 목소리를 합성해서 표현 가능한 것이 법적인 문제가 될 가능성은 없을까?심하게 규제를 하려 한다면 이런 합성 음성을 쓸 때마다 배경에 \\'가짜 음성입니다\\' 하는 메시지를 넣어야 하는게 의무화될 수도 있다. 하지만 그런 식으로 걸고 넘어진다면 어떤 기술도 시도를 할 수 없게 된다. 오히려 연예기획사 등에서 이런 기술에 호응하고 있다. 이를 이용해 새로운 비즈니스 기회를 만들 수 있기 때문이고 실제로 논의도 진행하고 있다. 다소 논란이 될 수 있으나 이는 시장 초기라 그런 것이며, 조만간 사회적으로 합의점이 도출될 것으로 보인다.Q. 음성 콘텐츠의 매력은 무엇이며 그 미래에 대한 예상은?음성 콘텐츠는 영상 콘텐츠와 달리 멀티태스킹(다중작업)이 가능하다. 음성 방송을 들으며 글을 읽거나 공부를 할 수도 있고 운전이나 운동도 가능하다. 사용자의 시간을 빼앗는 영상과 달리, 음성 콘텐츠는 시간의 낭비를 최소화할 수 있다. 라디오가 아직도 생명력을 유지하는 것도 그런 이유다. 그리고 세상에 가장 많은 콘텐츠가 아직 문자 기반이다. 이를 멀티미디어화 하는데 타입캐스트가 그 시발점이자 관문이 될 것이다. 또한 영상 콘텐츠를 제작하는 데도 음성이 필요하기 때문에 타입캐스트의 가능성은 무궁무진하다.네오사피엔스 김태수 대표 (출처=IT동아)그리고 우리는 향후 실시간으로 음성 대화하는 기술의 개발에 무게를 두고 있다. 시리나 빅스비 같은 AI 서비스가 이미 있지만 아직도 기계적인 느낌이 강하다. 우리의 기술이 이런 음성 AI에 마치 살아있는 것처럼 영혼을 불어넣을 수 있다. 또한 AI 기반의 디지털 인간을 만들고자 하는 기업들이 많은데 이들은 목소리가 없어서 고민을 하고 있다. 이러한 4차 산업혁명 분야에 우리의 기술이 큰 도움이 될 것이다.동아닷컴 IT전문 김영우 기자 pengo@donga.com오늘의 핫이슈창닫기기사를 추천 하셨습니다네오사피엔스 김태수 \"영혼 느껴지는 \\'AI 성우\\' 현실화\"베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( \\'#bestnews_layer\\' ), {\\r\\n                    vertical: \\'center\\',  /* top, bottom, center */\\r\\n                    horizontal: \\'center\\',  /* left, right, center */\\r\\n                    //effect: \\'slide\\',  /* clip slide blind */\\r\\n                    //direction: \\'left\\',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });',\n",
       " \"최진희 서울시립대 교수, ‘독성 화학물 예측’ 연구 발표데이터 모아 딥러닝으로 분석지금까지 1400명이 넘는 사망자가 발생한 것으로 알려진 가습기 살균제 사건은 화학물질의 위험성을 온 사회가 뼈저리게 느낀 계기였다. 8월 말 열린 ‘가습기 살균제 참사 진상규명 청문회’ 등을 통해 원인이 밝혀졌듯이 문제의 물질인 ‘폴리헥사메틸렌구아니딘(PHMG)’과 ‘클로로메틸이소티아졸리논/메틸이소티아졸리논(CMIT/MIT)’의 흡입독성 평가가 제대로 이뤄지지 않은 게 근본적인 원인이다.전문가들은 가습기 살균제 원인물질을 특정 용도에 대해서만 평가했고 흡입독성 평가가 제대로 이뤄지지 않았다고 지적했다. 재발 방지를 위해서는 흡입독성을 유발할 가능성이 있는 화학물질에 대한 실험과 실험을 할 수 있는 인프라가 필요하다. 보통 동물실험을 통해 흡입독성을 확인하는데 이 경우 시간과 비용이 많이 든다. 게다가 독성에 대한 동물실험은 동물복지에 반한다는 이유로 금지하거나 줄이려는 게 세계적인 추세다. 2009년 유럽연합(EU)의 화장품 원료에 대한 동물실험 전면 금지와 2016년 동물실험 화장품 유통·판매를 원칙적으로 금지한 국내 화장품법 개정이 대표적인 사례다.최진희 서울시립대 환경공학부 교수(사진)는 전 세계에 흩어져 있는 광범위한 화학물질 데이터베이스와 미국 환경보호국(EPA)의 독성 빅데이터 ‘톡스캐스트’를 토대로 인공지능(AI) 기술인 ‘딥러닝’을 적용해 특정 화학물질의 흡입독성을 동물실험 없이 예측할 수 있는 연구 결과를 발표해 세계적인 주목을 받고 있다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });5월 미국화학회(ACS)가 발간하는 학술저널 ‘독성학화학연구’에 발표된 연구논문은 ‘케미컬 워치’ 등 화학물질 관련 주요 해외 소식지에 소개됐다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화화학물질 빅데이터와 동물실험 독성 데이터를 바탕으로 독성을 예측하는 AI가 처음 개발된 것은 아니다. 토머스 하퉁 미국 존스홉킨스대 교수 연구팀은 지난해 7월 EU가 규제하고 있는 화학물질 약 1만 종의 동물실험 독성데이터를 바탕으로 독성예측 모델을 만들어 국제학술지 ‘독성과학’에 발표해 반향을 일으켰다. AI를 이용하면 동물실험을 대체할 수 있다는 연구였지만 동물과 사람의 화학물질 반응 메커니즘이 다르다는 점에서 완벽하게 동물실험을 대체하기는 어렵다는 평가도 있었다.최 교수 연구팀의 접근법은 달랐다. 화학물질과 최종 독성의 상관관계만을 분석하는 게 아니라 ‘독성발현경로(AOP)’ 개념을 접목했다. 화학물질이 흡입됐을 때 생체 내에서 어떤 과정을 거쳐 독성이 발현되는지 단계별 메커니즘을 찾아 화학물질의 흡입독성을 예측하는 것이다.이를 위해 연구팀은 전 세계에 쌓여 있는 화학물질 데이터베이스를 분석해 흡입할 시 유해성이 있는 화학물질 654개를 선별하고 톡스캐스트에서 AOP와 관련된 25개의 독성 데이터를 딥러닝 알고리즘에 적용해 독성 예측 모델을 만들었다. 그 결과 10여 개의 흡입독성 유발 물질을 선별하는 데 성공했다. 흡입했을 때 어느 단계에서 어떤 메커니즘으로 독성이 발현되는지 찾아낸 것이다.최 교수는 “우선순위 후보군으로 설정한 10여 개의 물질이 실제로 흡입독성을 유발하는지 동물실험을 통해 검증해야 한다”며 “동일한 독성 동물실험이 전 세계에서 반복적으로 이뤄지고 있는 상황에서 이번에 내놓은 연구 결과는 동물실험 자체를 줄이는 동시에 동물실험에 필요한 시간과 비용을 효율화하는 데도 큰 역할을 할 것”이라고 설명했다.연구팀은 현재 이슈로 부각된 미세플라스틱과 미세먼지에 대해서도 동일한 AOP 개념을 접목해 연구를 진행할 계획을 세웠다. 최 교수는 “화학물질 독성평가 인프라를 갖추는 데 많은 비용이 드는 만큼 데이터와 AI 기술을 활용해 동물실험을 최소화하고 효율적으로 인프라를 갖추기 위한 방안이 될 것”이라고 말했다.김민수 동아사이언스 기자 reborn@donga.com창닫기기사를 추천 하셨습니다가습기 살균제 흡입독성 실험 AI 활용해 동물 희생 줄인다베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"㈜수아랩수아랩의 SuaKIT Boxes.최근 베트남이 글로벌 제조업체의 생산기지로 급부상하고 있다. 베트남 국민 특유의 근면성과 저렴한 인건비에 최근 베트남 정부의 해외투자자금 유치를 위해 파격적인 세제 혜택이 더해지면서 전 세계 제조업 공장을 블랙홀처럼 빨아들이는 모양새다. 과거 섬유, 의류에 집중된 진출 영역도 스마트폰 제조 등 고부가가치 산업으로 확대되고 있다. 국내에서도 삼성, LG, SK 그룹 내 대형 전기전자 업체들이 베트남 현지생산 확대에 박차를 가하고 있다.베트남 현지공장 운영… 품질관리 등 개선과제하지만 생산기지를 베트남으로 옮긴 이후에도 운영효율성을 극대화하기 위해서는 많은 노력이 필요하다. 가장 문제가 되는 부분은 품질관리다. 하루에도 수십만 단위의 제품이 생산되는 전기·전자업계의 특성상, 품질관리 기준 및 인력의 전반적 숙련도 관리가 잘 이뤄지지 않으면 효율성 극대화가 어렵다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });4월 발표된 ‘베트남 기업 생산성 및 경쟁력 보고서’에 따르면 베트남 제조업의 노동생산성은 한국과 일본의 7% 수준에 불과한 것으로 나타났다. 또 최근 5년간 7%를 웃도는 인건비 상승률도 장기적으로 대비해야 할 부분이다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화여기에 대표적으로 해당되는 공정이 ‘외관검사’ 영역이다. 첨단산업으로 갈수록 제조공정이 자동화되는 추세이지만, 유독 외관검사는 ‘사람의 눈’을 여전히 필요로 한다. 사람이 판단하기에도 애매한 감성불량이 많아 기존에 있는 머신비전 기술로는 검출하기 어려운 부분이 있기 때문이다.최근 인공지능 기술을 활용해 ‘사람의 눈’을 대체하는 솔루션을 베트남에서도 도입하려는 움직임이 확대되고 있다. 라인을 가동하면서 이미 확보된 불량 이미지를 인공지능망으로 학습을 시키면 스스로 기준을 만들어 불량 검출을 자동화한다. 사람이 검사할 때보다 일관성 있는 검사가 가능할 뿐만 아니라 검출력도 높일 수 있다. 또 사람이 해야 할 단순 반복 작업량을 줄여주거나 완전히 작업자를 대체할 수 있어 소인화 효과도 기대할 수 있다.인공지능 기반 외관검사 자동화 기업국내 딥러닝 벤처기업인 ㈜수아랩(대표 송기영)은 이러한 수요에 대응하기 위해 공격적으로 베트남 진출에 박차를 가하고 있다. 2017년 중반부터 상용화를 시작했지만 빠르게 해외를 공략하면서 중국, 일본, 동남아 포함 26개국 대상 해외 매출 비중이 전체 매출의 절반을 넘는다. 이미 동남아에 진출한 국내 전기전자 업체와 이들의 1, 2차 협력업체 대상으로도 활발하게 협업을 진행 중이다.소위 인공지능 붐이 일면서 다수의 업체들이 생겨나고 있지만 기존 작업방식을 바꾸면서까지 인공지능 솔루션이 도입되는 데에는 어려움이 따른다. 딥러닝 학습에 필요한 학습 샘플을 확보하는 단계부터 검출 성능을 달성하기까지 여러 가지 실무적인 어려움에 봉착한다. 솔루션을 도입하기 위한 환경이 갖춰지지 않았거나, 학습에 필요한 불량 이미지 수가 충분하지 않거나, 새로 매번 학습을 시키기에는 제품의 교체주기가 짧은 경우 등 수많은 현장 변수가 존재한다.현장특화-뛰어난 기술지원-지속적 R&D 3박자 갖춰수아랩의 핵심 경쟁력 포인트는 무엇일까. 첫 번째는 현장 적용을 위해 특화된 기능이다. 대표 소프트웨어 제품인 ‘수아킷’에는 여러 가지 현장 적용에 특화된 기능이 탑재돼 있다. 학습시킬 불량 이미지가 부족한 경우 정상 이미지만으로 학습을 할 수 있거나(One Class Learning), 검사 제품군의 교체주기가 짧은 경우 제품 간 차이점만을 학습시켜 빠르게 대응할 수 있는 Image Comparison 등 다양한 현장 적용 기술을 지속적으로 개발하고 제품에 탑재한다.다른 한 가지로는 현장 기술지원 역량을 꼽을 수 있다. 인공지능 도입을 위한 환경 조성 작업부터 검출 기준 및 목표 설정, 망 성능의 최적화까지 고객사 환경에 따라 도입에 필요한 여러 해결방법을 제공하는 기술지원이 업계 내 독보적이라는 평가를 받고 있다.수아랩 사업개발을 총괄하고 있는 문태연 부대표는 “유명한 고객사들과 프로젝트를 진행하면서 쌓은 노하우를 바탕으로 베트남에 진출한 글로벌 기업의 검사 자동화를 도울 것”이라며 “최근 신속한 도입을 위해 국내 본사 검증과정을 생략하고, 곧바로 베트남 현지 공장에서 파일럿 테스트를 진행하기를 원하는 사례가 늘고 있다”고 밝혔다.한편 수아랩은 최근 성능을 대폭 개선시킨 수아킷 버전 2.3을 출시했다. 국내 최대 중소기업, 스타트업 발굴 행사인 ‘제1회 대한민국 중소기업, 스타트업 대상’에서 중소벤처기업부 장관상 스타트업부문을 수상하며 다시 한 번 기술력을 입증받은 수아랩의 귀추가 주목된다.정상연 기자 j301301@donga.com창닫기기사를 추천 하셨습니다현장적용에 특화 딥러닝 알고리즘 ‘수아킷’… 베트남 진출 박차베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"트위그팜은 아시아 최고수준의 자연어처리(NLP) 전문 연구소가 되기 위해 매진하고 있다. 사진은 트위그팜 백선호 대표와 연구소 단체 사진.구글, 마이크로소프트 등 글로벌 기업이 선도하는 인공지능 기계번역 시장에서 국내 스타트업 기업인 ㈜트위그팜(대표 백선호)이 화제다. 트위그팜은 법률 분야 한영기계번역에서 구글을 능가하는 점수를 기록해 새로운 틈새시장(Niche Market) 발굴에 일조했다는 평가를 받고 있다.자연어에 가까운 인공신경망 기반 혁신기술트위그팜은 전문 번역가와 딥러닝 기반의 자연어처리(NLP) 기술을 통해 인공지능 전문 번역 솔루션을 제공하는 기업이다. 최근 한국정보통신기술협회(TTA) 소프트웨어시험인증연구소 확인 및 검증시험(Verification & Validation)을 통해 법률 맞춤형 기계 번역기의 성능을 구글과 비교한 결과 구글보다 높은 점수를 기록했다. 실제 번역 결과와 비교해 용어의 정확도, 표현의 적절성을 분석해 산출한 점수에서 트위그팜의 기계 번역이 좋은 점수를 얻었다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });트위그팜 백선호 대표는 “법률 분야의 한영 번역 부분이라고 해도 트위그팜이 자체적으로 개발한 인공지능 기술력의 확장 가능성과 시장 경쟁력을 증명하는 바라서 그 의미가 크다”고 말했다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화이전의 기계 번역이 이미 저장돼 있는 해석에 따라 기계적인 번역을 제공하는 것에 그쳤다면 인공신경망 번역은 전후 문맥 상황을 파악해 가장 적절하고 흐름에 맞는 번역을 찾아주는 보다 정교한 번역이다. 따라서 인공신경망 기반 인공지능 기술은 머신 러닝 기술보다 한 차원 진화된 기술인 셈이다.21개국 언어 서비스 가능, 전문 분야로 확대현재 트위그팜은 인공지능 기반 온라인 번역 플랫폼인 ‘지콘스튜디오(Gcon Studio)’를 통해 한영, 영한 번역뿐 아니라 중국어, 일본어, 러시아어 등 21개국의 언어 서비스를 제공하고 있다. 전문 번역 분야 또한 화학, 바이오 테크, 특허 등 다양한데, 인공신경망 번역 기술력을 다른 언어로 계속 확장해 나갈 예정이다. 이에 대해 홍경수 수석연구원은 “트위그팜은 전문 분야별로 양질의 데이터를 구축하고 있고, 한국어를 잘 이해하는 알고리즘에 집중하고 있다”며 “이번 결과물은 세계적인 경쟁력을 입증하는 동시에 앞으로 데이터 축적을 통해 전문 분야에서 정확도는 물론 신뢰도 높은 번역 서비스를 제공하게 될 것으로 기대한다”고 말했다.정확성을 기하려면 전문 용어, 표현의 맞춤형 데이터베이스인 ‘번역 사전’의 역할이 크다. 그만큼 전문 용어집과 번역 노하우가 담긴 번역 메모리, 경험을 통해 습득한 언어 패턴의 빅데이터화가 요구된다. 이를 위해 인공지능 기반의 온라인 플랫폼인 지콘스튜디오를 통해 기업·언어·분야별 맞춤 번역 서비스를 제공하고 있다. 지속적인 재사용도 가능해 번역 비용을 약 30∼80%까지 절감할 수 있다.백 대표는 “전 세계 기업들이 글로벌 진출을 꿈꾸는 만큼 언어가 경쟁력 확보에 걸림돌이 되지 않도록 돕는 것이 목표”라며 “전문 분야에서 상호 간 비즈니스 번역은 일반 번역과는 결이 다르기 때문에 정확한 번역 솔루션을 제공하기 위해 기술력을 총동원할 것”이라고 말했다.트위그팜은 올해 전 세계 최고 특허 회사인 Morningside Translation과 계약을 체결하고, 인공지능 기반 기계 번역과 전문 번역가 리뷰를 결합한 형태인 MTPE(Machine Translation Post-Editing) 특허번역 서비스를 제공하고 있다. 최근에는 한국데이터산업진흥원에서 진행하는 데이터 바우처 지원사업의 데이터가공업체로 선정되기도 했다.박정민 기자 atom6001@donga.com창닫기기사를 추천 하셨습니다신경망 기계번역 ‘트위그팜’, 구글 뛰어넘다… 표현력-정확성 점수 앞서베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"㈜콘텐츠오션(대표 송창화)은 서울대학교 수리과학부 수치계산 및 영상분석 연구실(NCIA·교수 강명주), ㈜에이아이네이션(대표 곽지훈)과 지난 17일 산학협력 양해각서(MOU)를 체결했다고 18일 밝혔다.콘텐츠오션에 따르면 이번 업무협약은 ▲인공지능(딥러닝) 기반 방송 스튜디오인 ‘AI Studio’ 구성 기술 공동 연구 개발 ▲인공지능 촬영 및 편집 기술 공동 연구 개발 ▲AI Studio 구축을 위한 시범 콘텐츠 제작(영상 콘텐츠 중심) 등이 골자다.참가 주체를 살펴보면, 먼저 콘텐츠오션은 통합마케팅과 브랜딩, 디지털미디어 등을 제공하는 방송·영상 콘텐츠 제작 전문기업이다. 지상파와 종편 등 방송 제작 및 바이럴광고 제작, 네이버 VLive, MCN, LTE 중계 등 온라인 방송을 비롯해 방송 협찬, 매체 대행, 1인 크리에이터 교육 아카데미 사업 등을 펴고 있다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });에이아이네이션은 산업인공지능 전문기업이며, 서울대학교 수리과학부 NCIA 연구실은 사물이나 데이터를 군집화 하거나 분류하는데 사용하는 딥러닝(Deep Learning)을 기반으로 한 영상분석 및 AI 전문 연구소다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화특히 서울대학교 수리과학부 NCIA 연구실은 지난 2016년 응용수학 분야의 세계적인 석학 스탠리 오셔 교수와 ‘딥러닝에 대한 수학적 연구’를 진행했으며, 올 초에는 국내 대표적인 1인 미디어와 함께 영상 콘텐츠를 딥러닝 기술로 분석해 유저 맞춤형 AI 서비스를 개발에 관한 업무 협약을 체결하는 등 딥러닝 기술을 활용한 서비스 개발에 박차를 가하고 있다.콘텐츠오션과 서울대학교 수리과학부 NCIA 연구실, 에이아이네이션은 이번 업무협약을 통해 AI Studio 연구 개발에 힘을 합쳐 다양한 산업에 응용해 일자리 창출 및 경제 활성화 기여하겠다고 포부를 밝혔다.콘텐츠오션 송창화 대표는 “에이아이네이션과 서울대학교 수리과학부 NCIA 연구실과의 협력으로 인공지능 딥러닝을 활용한 AI Studio 산업 발전에 앞장설 수 있게 됐다”며 “AI Studio 구축을 위한 콘텐츠 제작에 적극적으로 협조하겠다”고 말했다.박해식 동아닷컴 기자 pistols@donga.com오늘의 핫이슈창닫기기사를 추천 하셨습니다콘텐츠오션·서울대 NCIA·에이아이네이션, ‘AI 스튜디오 개발’ 산학협력 MOU베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"차세대 병원정보시스템 AMIS 3.0 개선 통해 헬스케어 사업 지원엠투아이티의료IT기업 ㈜엠투아이티(M2IT)가 최근 서울아산병원과 차세대 병원정보시스템인 AMIS 3.0 개선 용역 계약을 체결했다고 밝혔다. 서울아산병원은 2017년부터 현대오토에버와 손잡고 의료전산시스템 AMIS 3.0 서비스를 구축한 바 있다.서울아산병원은 차세대 병원정보시스템 AMIS 3.0을 통해 병원 내 의료데이터 표준화를 완성하고, 해당 데이터에 기반해 맞춤형 및 질병예측형 진료를 확대해 나가겠다고 밝힌 바 있다. 또한 AI, 헬스케어 데이터를 활용해 개인별 맞춤형 진료를 선보이며 병원 경쟁력을 높일 것으로도 기대하고 있다.AMIS 3.0 시스템의 유지보수를 담당하게 된 엠투아이티는 이번 계약을 토대로 개선된 환경에 기반해 보다 안정적으로 헬스케어 사업을 운영토록 지원하게 됐다. 서울아산병원은 로봇, 인공지능 가상 증강현실 등 4차산업혁명 핵심 기술을 임상에 적용해 나가기로 밝힌 바 있다.엠투아이티는 의료IT 기술의 새로운 시각과 방향성을 바탕으로 차세대 의료정보시스템을 선보이고 있는 IT전문기업이다 .나아가 의료분야에 국한되지 않고 빅데이터나 AI, 머신러닝과 딥러닝 쪽에도 접근해 새로운 IT 기술 및 Trend를 기초로 한 혁신적인 기술을 선보이기 위해 다양한 활동을 전개 중이다.엠투아이티 관계자는 “최근 국내 대형병원들이 빅데이터, 인공지능 등 4차산업혁명 기술에 기반해 새로운 시스템을 도입하고 있으며, 해당 분야에서 기술적 우위를 점하기 위해 그 경쟁이 치열해지고 있는 상황”이라며 “이번 서비스 개선 용역 계약을 통해 서울아산병원만의 특화된 의료사업을 환자들에게 선보이도록 적극 지원할 것”이라고 밝혔다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });한편 엠투아이티는 4차산업혁명 관련 기술 보유 업체 연합체인 ‘이노퓨처얼라이언스’에 참여해 변화하는 시장에 공동 대응하고 있다. 또한 산업자원통상부의 국책연구과제에 투입돼 의료데이터 관리 표준화와 이를 통한 빅데이터 구축 및 활용에 대한 시스템 구축을 수행한 바 있다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화동아닷컴 최용석 기자 duck8@donga.com창닫기기사를 추천 하셨습니다엠투아이티, 서울아산병원 AMIS 3.0 서비스 개선 용역계약 수주베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"민원기 과학기술정보통신부 2차관(왼쪽)과 이성환 고려대 인공지능(AI)학과 교수가 5일 서울 성북구 고려대 미래융합기술관에서 AI대학원 현판을 들어 보였다. 고려대 제공국내 첫 인공지능(AI)대학원이 5일 서울 성북구 안암동 고려대 미래융합기술관에서 개원 기념식을 갖고 출범했다.고려대 일반대학원 산하 AI대학원은 매년 석·박사 통합 및 박사과정 신입생 50명을 모집할 계획이다. 주 연구 분야는 딥러닝, 음성인식, 빅데이터 등이며 헬스케어, 금융, 자율주행 같은 특화 연구도 진행한다.AI대학원은 미국 카네기멜런대(CMU)와 매사추세츠공대(MIT), 독일 막스플랑크연구소를 비롯한 세계 유수의 대학 및 연구소 15곳과 공동 연구를 추진해 기술창업 인재를 양성한다. 구글 페이스북 삼성전자 같은 글로벌 기업 38곳과 산학 협력해 대학원생들의 인턴십을 의무화하기로 했다. 2028년까지 우수 벤처 기업 10곳 배출을 목표로 한다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });정진택 고려대 총장은 이날 기념식 환영사에서 “AI 관련 모든 대학, 연구소, 산업체와 협업해 한국이 세계 최고의 AI 인재를 양성하는 나라가 될 수 있도록 지원하겠다”고 말했다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화이날 기념식에는 민원기 과학기술정보통신부 2차관, 김태희 서울시 경제일자리기획관, 유승희 더불어민주당 의원, 석제범 정보통신기획평가원 원장 등 약 200명이 참석했다.박재명 기자 jmpark@donga.com창닫기기사를 추천 하셨습니다高大, 국내 첫 AI대학원 개원 “창업인재 양성”베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"인공지능(AI)·데이터 과학 전문 기업 솔트룩스가 ‘설명가능한 인공지능과 지식그래프’를 주제로, 인공지능 최신 트렌드와 기술을 접할 수 있는 원데이(1 Day) 세미나&튜토리얼 행사를 진행한다고 5일 밝혔다.행사는 오는 19일 서울 강남구 테헤란로 스페이스쉐어 대치센터 5층 펜타곤홀에서 열린다.‘설명가능한 인공지능(eXplainable AI·XAI)’이란 말 그대로 AI의 사고 과정을 들여다볼 수 있게 하는 기술이다. 인공지능이 판단한 이유를 사람이 이해할 수 있는 방식으로 제시하는 인공지능을 가리킨다. 특정 판단에 대해 알고리즘의 설계자조차 이유를 설명할 수 없는 ‘블랙박스’ 인공지능과 대비되는 개념으로, 최근 인공지능 분야의 최대 화두 중 하나이다.전문가들에 따르면 이러한 설명가능한 인공지능을 위해서는 딥러닝 기술과 인간에게 설명 가능성을 확보하기 위한 기호적 접근 방식의 인공지능의 결합이 매우 중요하다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });솔트룩스는 이번 행사에서 설명가능한 인공지능 관련 기술을 소개할 예정이다. 이 업체는 ‘앙상블 AI’ 관련 특허기술을 보유하고 있다. 앙상블 AI는 심층신경망(딥러닝)과 지식그래프 등 다양한 AI 기술의 융합을 통해 보다 높은 성능을 도출하는 인공지능 기술이다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화특히 IT 시장조사기관 가트너에서 향후 3~5년 간 파괴적인 영향력을 미칠 기술 트렌드에 선정된 지식그래프(Knowledge Graph)와 그래프DB(Graph DB)도 함께 다룰 예정으로, 앞으로의 인공지능 트렌드를 이끌어갈 최신 기술들을 공유하는 자리가 될 것으로 보인다.‘지식의 초연결–연결된 지식이 바꾸는 세상’이라는 주제로 솔트룩스 이경일 대표의 키노트 발표가 진행되며, 국내 대표적 인공지능 전문가인 카이스트 최기선 교수의 초청강연도 예정 돼 있다. 또한 오후에는 솔트룩스의 지식그래프/그래프DB 담당 실무자들과 함께 참가자들이 직접 실습해보는 ‘Hands-on’ 세션이 마련되어 있어 솔트룩스의 최신 AI기술을 직접 경험해 볼 수 있는 시간으로 꾸며질 예정이다.행사는 유료로 진행되며 벤처/스타트업 및 대학(교수/학생) 참가자는 참가비 50% 할인, 솔트룩스와 업무 제휴/협약을 맺은 비즈니스 파트너사 임직원은 3명까지 무료로 참석이 가능하다고 업체 측은 밝혔다. 온라인 사전신청은 17일까지 가능하며, 솔트룩스 홈페이지를 통해 확인할 수 있다.박해식 동아닷컴 기자 pistols@donga.com오늘의 핫이슈창닫기기사를 추천 하셨습니다솔트룩스, ‘XAI와 지식그래프’ 세미나&튜토리얼 행사 개최베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"개강 앞둔 ‘AI대학원’ 3곳 살펴보니인공지능(AI) 분야 고급 인력을 키우기 위해 설립된 AI대학원이 이번 주 첫 신입생을 맞으며 수업을 시작한다. 동아일보DB8월 26일 KAIST가 인공지능(AI)대학원을 정식 개원하는 것을 시작으로 고려대와 성균관대도 이달과 다음 달 잇따라 AI대학원을 개설한다. 과학기술정보통신부는 3월 KAIST를 비롯해 고려대와 성균관대를 AI대학원 지원 사업 대상 학교로 선정했다. 이들 대학은 5년 동안 총 90억 원의 정부 예산을 받아 미래 AI 및 자율주행, 데이터 과학 연구를 선도할 인재를 키운다. 본격적인 개강을 앞둔 3개 AI대학원의 전임교수 및 개설 교과목 수 등 강점과 준비 현황 등을 점검했다.KAIST는 개설 과목 수가 가장 많다. 성균관대는 확보된 전임교수가 가장 많고, 고려대는 AI 기업과의 산학협력이 강한 것이 특징이다. KAIST는 현재 8명의 전임교수진과 18개 과목으로 구성된 커리큘럼을 준비했다. 전임교수는 내년 봄 2명이 더 충원돼 총 10명이 될 예정이다. 4월 석사과정 22명과 박사과정 10명 등 첫 신입생을 모집했다. 2020년 봄학기 신입생 역시 7월 모집이 끝났다. KAIST는 내년부터 매년 석사과정 40명과 박사과정 20명 이상을 모집할 계획이다.KAIST는 평균 나이 41세의 비교적 젊고 연구 역량이 좋은 학자들로 전임교수진을 꾸렸다. KAIST는 “교수진이 최근 6년 동안 AI 분야 주요 학회에서 101편의 논문을 발표하는 등 최근 AI 분야에서 활발히 활동하고 있다”며 “기계학습 및 신경정보 처리 시스템 분야 학회에서 우수한 연구 성과를 보인 한국인 학자 10명 가운데 3명이 KAIST AI대학원 교수진”이라고 밝혔다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });고려대는 이달 5일 개원 기념식을 열고 공식적인 운영에 들어간다. 고려대는 전임교수 7명을 배치했다. 이번 첫 학기에는 AI의 기본 학습 기능인 딥러닝(심화학습)부터 자연어 처리, 신경망, 빅데이터와 음성 인식 등 기초부터 응용까지 포괄하는 필수 및 선택과목 9개를 개설했다. 이성환 주임교수는 “다음 학기부터는 과목 수를 더 늘릴 계획”이라고 밝혔다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화고려대는 특히 국내외 AI 기업과의 네트워크를 충분히 이용해 실무 역량을 키운다는 계획이다. 이 교수는 “삼성전자와 NC소프트, 넷마블 등으로부터 지원을 받으며 실질적인 교류를 시작했다”며 “이론과 연구도 중요하지만 산업체와 현장에서 필요로 하는 기술을 습득한 고급 인력을 키우는 데 특히 중점을 뒀다”고 말했다. 이를 위해 특이하게 석사과정을 뽑지 않고 연간 50명의 정원 전원을 박사 및 석박사 통합과정으로 선정할 계획이다. 2020년도 신입생은 고려대 일반대학원생 모집 일정에 맞춰 이달 말에 공지가 나가고 10월 첫 주에 원서 접수를 시작한다.성균관대 역시 가을학기부터 수업을 정상적으로 진행하지만, 개원식은 다소 늦은 10월 열기로 했다. 성균관대는 세 대학원 가운데 가장 많은 총 15명의 전임교수가 총 12과목의 전공과목을 맡았다. 첫 학기 신입생으로 50명의 학생을 이미 선발했다. 이달부터는 2020년 봄학기 학생 모집 공고를 시작한다.성균관대는 연구 참여 교수와 별도로 아예 국제협력과 산학협력을 맡을 초빙교수를 둔 게 특징이다. 학계와 산업이 긴밀하게 협력해야 하는 AI 분야의 특성을 고려해 연구 참여 교수의 행정 및 사업 부담을 줄이기 위해서다. 최재영 국제협력 및 산학협력담당 교수(초빙교수)는 “첫 학기를 지내며 역량을 강화할 분야를 파악해 해당 분야 전문 초빙교수를 추가로 모집할 계획도 있다”며 “전임교수가 연구와 교육에만 전념하게 해 연구와 교육의 질을 높일 것”이라고 말했다.이 3개 대학의 AI대학원은 연 50∼60명 정원으로 독립된 학과 형태로 운영된다. 반면 해외 주요 AI 대학원은 컴퓨터과학이나 로봇공학 등 다양한 분야와 결합해 연구 범위도 넓고 규모도 훨씬 크다. 미국 매사추세츠공대(MIT) 컴퓨터과학 및 인공지능연구소(CSAIL)는 교수 122명에 대학원생이 500명 재학하고 있다. 행정직원을 포함해 총 구성원이 약 1000명에 이르는 거대 조직이다. 규모만으로 국내 AI 대학원의 10배가 넘는다. AI 분야의 또 다른 대표적 강자인 미국 카네기멜런대 로봇연구소(RI) 역시 교수 92명에 학생 수가 400명이 넘는다. 총 구성원은 약 950명이다. CSAIL은 컴퓨터과학, RI는 자율주행과 로봇 AI에서 두각을 나타내고 있다. 정부는 2020년 봄학기 개원을 목표로 2기 AI대학원 지원 사업에 참여할 대학을 공모할 예정이다. 1기에 참여하지 못한 서울대 등 일부 대학이 참여를 고려 중인 것으로 알려져 있다.윤신영 동아사이언스 기자 ashilla@donga.com창닫기기사를 추천 하셨습니다한국의 미래 이끌 ‘AI 대학원’ 어디가 좋을까베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"AI-인간 변호사 ‘알파로’ 대결, 본보 기자가 직접 참가해보니29일 오후 서울 서초구 서초동 변호사회관에서 열린 ‘제1회 알파로 경진대회’에서 인공지능(AI) 변호사와 인간 변호사들이 법률 자문 대결을 벌이고 있다. 12개 팀 중 AI와 짝을 이룬 3개 팀이 1∼3등을 모두 차지해 변호사들로만 구성된 9개 팀을 꺾었다. 신아형 기자 abro@donga.com“변호사 자격이 없는 참가자가 법률 자문 대결에서 변호사를 꺾었습니다!”29일 서울 서초구 서초동 변호사회관. 경기 결과가 발표되자 장내가 술렁이며 곳곳에서 탄성이 터져 나왔다. 자격증만 없을 뿐 재야의 고수였을까. 아니다. 법률 지식이 없는 일반인이 인공지능(AI)의 도움을 받아 변호사를 이긴 것이다. 대법원 사법정책연구원과 한국인공지능법학회 주최로 아시아 최초로 AI와 변호사가 대결한 ‘제1회 알파로(Alpha Law) 경진대회’였다.대회에는 12개 팀이 참가했다. AI와 변호사가 짝을 이룬 2개 팀, AI와 일반인이 힘을 합친 한 팀 등 3개 팀이 AI의 조력을 받았다. 나머지 9개 팀은 변호사 2명씩 팀을 이뤘다. 본보 신아형 기자가 유일하게 일반인으로 참가해 변호사들과 대결을 펼쳤다. 결과적으로 AI 팀이 1∼3위를 휩쓸며 압승을 거뒀다. 신 기자는 AI와 변호사가 연합한 2개 팀에 이어 3위를 차지했다. 기자의 점수는 150점 만점에 107점으로, 1위(120점)와는 13점밖에 차이가 나지 않았지만 4위를 차지한 변호사팀(61점)과는 압도적인 차이를 보였다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });근로계약서 3종의 오류와 누락, 위법요소를 분석해 대안을 제시하는 것이 과제였다. 대결에 사용된 AI는 인텔리콘 연구소가 개발한 ‘CIA(Contract Intelligent Analyzer)’였다. CIA는 컴퓨터가 판결문 등 외부 데이터를 분석해 스스로 성장하는 ‘딥러닝’ 방식을 채택한 노동법 전문 ‘AI 변호사’다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화심사위원장이 법봉을 두드리면서 경기가 시작됐다. 기자는 AI 변호사가 설치된 노트북에 근로자의 생년월일과 성별, 계약형태를 써 넣었다. 이어 근로계약서 파일을 첨부해 넣자 노트북에서 “삐리삐리” 소리가 났다. AI는 단 6초 만에 검토 결과를 보여줬다. 대학에서 물리학을 전공한 기자는 지금껏 법전(法典) 한 번 펼쳐본 적이 없지만 AI가 내놓은 답안을 옮겨 적는 건 간단한 일이었다.속도만 빠른 게 아니었다. AI는 계약서의 문제점을 귀신같이 잡아냈다. 계약서상 미성년자인 A 씨의 근로시간은 오후 3시부터 11시까지로 적혀 있었다. AI는 미성년자가 야간에 근로하지 못하도록 한 법령을 제시하며 문제라고 지적했다. “사용자는 18세 미만자를 오후 10시부터 오전 6시까지의 시간에 근로시키지 못한다”는 설명까지 내놨다.반면 변호사팀들은 문제당 20분씩 주어진 촉박한 시간에 당황한 모습이 역력했다. 구글, 네이버 검색과 스마트폰을 동원해 검색을 하고 빨간펜으로 답안을 썼다 지우기를 반복했다. AI 변호사의 장점인 속도와 정확성이 드러나는 순간이었다.대회 심사위원장을 맡은 이명숙 변호사는 “이번 대회는 변호사와 AI의 대결이 아니라 협업 가능성을 살펴보기 위해 마련됐다”며 “법률 AI가 발달하면 변호사와 판사가 AI의 도움을 받아 변론과 판결을 하는 시대가 곧 도래할 수도 있다”고 말했다.AI 기술을 법률 시장에 활용하는 것은 이미 세계적인 추세다. 미국은 이른바 ‘리걸테크(Legal Tech)’ 관련 기업들이 100여 곳 성업하며 AI 변호사 시대를 준비하고 있지만 한국은 아직 초기 단계다.신아형 abro@donga.com·이호재 기자창닫기기사를 추천 하셨습니다AI 도움에… 법 문외한이 변호사 눌렀다베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"인공지능-빅데이터 등 신기술 접목… 첨단내시경 수술법 등 잇따라 개발29일 세브란스병원서 심포지엄 열려연세대 장양수 의대 학장(협약서 든 왼쪽)과 홍대식 공대 학장(협약서 든 오른쪽)을 비롯한 양 대학 교수진과 학생들이 29일 서울 신촌 세브란스병원에서 열린 공동 심포지엄을 마치고 기념촬영을 했다. 연세대 제공올 1월 강남세브란스병원 비뇨의학과 구교철 교수와 연세대 기계공학과 박노철 교수 연구팀은 콩팥과 방광을 잇는 요관의 결석을 제거하는 새로운 내시경 수술법을 개발했다. 내시경 삽입 압력을 낮추는 장비를 활용해 수술 안전성을 높이고 환자의 불편을 줄이는 방식이다. 이 수술법은 식품의약품안전처의 의료기기 승인을 받아 상용화를 앞두고 있다.연구의 시작은 2017년 열린 연세대 공대와 비뇨의학과의 공동 심포지엄이었다. 공대에서 “왜 힘들게 개발한 장비를 의사들은 사용하지 않느냐”며 불만을 드러내자 의대 교수들은 “현장 요구에 맞는 장비를 개발해 달라”고 요구했다. 참석자들은 첨단 의료기기 개발을 위해 의료 전문성과 공학 기술력을 결합한 공동연구를 추진하자고 뜻을 모았다.최근 인공지능(AI)과 빅데이터를 비롯한 4차 산업혁명의 신기술과 첨단 소재를 의료 현장에 접목하는 학문 간 융·복합 연구가 활발하다. 연세대는 2017년부터 의료연구단을 구성해 공동연구와 강의를 해왔다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });연세대는 지난 3년간의 성과를 집약하고 최신 연구 동향을 소개하기 위해 29일 서울 서대문구 연세대 세브란스 암병원에서 ‘4차 산업혁명 대비 융·복합 의료분야의 공동협력 증진을 위한 발전 협약식 및 공동심포지엄’을 열었다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화이날 심포지엄에서 방승민 연세대 의대 소화기내과 교수는 외부에서 조종하는 캡슐형 내시경을 선보였다. 기존 내시경과 달리 환자의 번거로움과 고통을 줄일 수 있고 모든 소화기관 검사가 가능하다. 진단도 1시간 내에 이뤄진다. 방 교수는 “의사와 공학자의 꾸준한 소통이 새로운 의료기기나 치료법 개발의 핵심”이라고 강조했다.의료영상 분야는 AI 활용이 무척 활발하다. AI가 외부 데이터를 분석해 스스로 성장하는 ‘딥러닝’ 기술을 통해 가장 정확한 진단을 내릴 수 있어서다. 융·복합 연구는 산업 경쟁력 강화에도 도움이 된다. 이준상 기계공학부 교수는 “일본과 독일 제품이 많은 광학장비 분야에서 꾸준히 연구해 대체 기기를 생산하는 것이 장기 목표”라고 말했다.연세대는 융·복합형 인재를 양성하기 위해 의대와 공대의 공동 강의 및 연구를 내년부터 정식으로 도입한다. 올 1학기에는 공대와 의대 교수 4명이 참여한 과목을 시범 운영했다. 혈관과 식도 등에 삽입하는 스텐트를 기존 빅데이터를 활용해 직접 설계하고, 3차원(3D) 프린터로 만들어내는 등 성과도 적지 않다.연세대 관계자는 “AI 활용에 익숙한 융·복합형 의료 인재에 대한 수요가 세계적으로 급증하고 있다”며 “AI 대학원을 중심으로 한국형 의료 기술 혁신에 앞장서겠다”고 밝혔다.박성민 기자 min@donga.com창닫기기사를 추천 하셨습니다연세대, 4차 산업혁명 대비한 ‘융·복합 의료기술’ 연구 활발베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"2018년 9월, 엔비디아의 차세대 게이밍 그래픽 카드인 지포스 20 시리즈가 공개됐다. 코드명 튜링(Turing) 아키텍처가 사용된 지포스 20 시리즈는 레이 트레이싱(Ray Tracing, 실시간 광선 추적) 기능과 DLSS(Deep Learning Super-Sampling, 딥 러닝 슈퍼 샘플링)가 적용됐고, 전작보다 더 높은 게이밍 성능을 발휘한다. 다양한 신기능과 고성능 GDDR6 메모리로 인해 가격이 크게 올랐지만, 2년 3개월 만에 나온 그래픽 카드라 교체 수요가 상당했다.엔비디아 RTX 공개. RTX 2080이 가장 먼저 출시됐다. (출처=IT동아)지포스 RTX 2080 공개 이후, RTX 2080보다 성능이 높은 RTX 2080 Ti, 이보다 아래 단계인 RTX 2070이 공개됐고, 2019년 1월에 RTX 2060이 출시됐다. 한 달 후 레이 트레이싱 기능이 제외된 GTX 1660 Ti, 1660, 1650까지 추가돼 라인업이 완성됐다. 게이머는 총 7개 그래픽 카드 중 본인이 원하는 수준에 맞는 그래픽 카드를 선택하면 됐다.그런데 지난 2019년 7월, 갑작스럽게 RTX 2080, 2070, 2060 슈퍼가 등장했다. RTX 슈퍼 시리즈는 경쟁사인 AMD 나비(Navi) 기반 '라데온 RX 5000' 시리즈를 견제하기 위해 더 많은 쿠다 코어와 동작 속도를 적용한 개선판이다. 엔비디아는 RTX 2060 슈퍼는 RTX 2060보다 최대 22%, 평균 15%정도 빠르며, RTX 2070 슈퍼도 RTX 2070 슈퍼보다 최대 24%, 평균 16% 빠르다고 밝혔다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });RTX 2070 디자인, 녹색으로 'SUPER'라고 각인돼있다. (출처=엔비디아)문제는 누구도 예상하지 못한 출시인데다가, 이전 버전을 단종시킴과 동시에 할인 판매를 시작했다는 것. 발표 직전 RTX 시리즈를 구매했다면 RTX 슈퍼 가격에 단종된 RTX를 구매한 셈이고, 이전에 구매한 사람도 6개월~1년만에 제품이 단종된 것이라 비난이 거셌다. 반면 그래픽 카드 구매를 미루고 있던 사람들에게는 호재다. 구매자들 간의 희비가 엇갈리는 만큼 엔비디아도 조용히 판매에 들어간 상황이다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화2019년 8월 말 현재, RTX 2080 / 2070 / 2060이 단종되고, RTX 2080 슈퍼 / 2070 슈퍼 / 2060 슈퍼가 그 자리를 대체했다. 단종된 제품은 7월 9일을 기점으로 약 10만 원씩 가격이 내려갔다. 새 제품이지만, 출시 초기에 발생하는 품귀현상 없이 곧바로 안정적인 가격을 유지하고 있다.RTX 슈퍼, 기존 RTX 시리즈와 다른 점은?RTX 2070 파운더스 에디션, RTX 2070, 2070 슈퍼 사양 비교 (출처=엔비디아)단종된 RTX 시리즈와 새롭게 그 자리를 꿰찬 RTX 슈퍼 시리즈 중 중간 성능을 발휘하는 RTX 2070 슈퍼와 단종된 RTX 2070을 비교해보자. 먼저 쿠다(CUDA) 코어 개수가 2,304개에서 2,560개로 증가했다. 쿠다 코어는 엔비디아의 데이터 연산 명령어인 쿠다에 최적화된 코어로, 많을수록 게이밍 및 그래픽 처리 성능이 향상된다.RTX-OPS도 42~45T에서 52T로 향상됐고, 초당 기가 레이(Giga Rays/s)도 6초에서 7초로 빨라졌다. 다만 이 단위는 엔비디아가 자체적으로 측정한 기준이니 RTX 시리즈에 탑재된 '실시간 광원 처리 성능이 발전했다' 정도로 보면 된다.부스트 클럭과 베이스 클럭도 차이를 보인다. 여기서 클럭은 그래픽 칩셋이 동작하는 속도를 의미하며, 높을수록 더 빠르게 데이터를 처리한다. RTX 2070 슈퍼는 베이스 클럭이 약 200MHz 향상돼 기본 처리 속도가 빨라졌고, 최대 속도는 비슷하다. 메모리는 GDDR6가 그대로 사용된다.RTX 2080 슈퍼 출시 (출처=엔비디아)결과적으로 RTX 슈퍼 시리즈는 튜링 아키텍처를 더 최적화한 제품이다. 이미 구매한 사람이라면 달갑지 않겠으나, 새로 구매하는 입장이라면 같은 가격으로 더 좋은 제품을 쓰게 됐다. 그렇다면 각 제품은 어떻게 구분할까?RTX 2060 슈퍼는 현존하는 모든 그래픽 카드와 비교해서는 상위급이지만, RTX 시리즈 중에서는 낮은 편이다. 레이 트레이싱이 목적이라면 향후에 성능이 부족할 수 있다. RTX 2080 슈퍼는 게이밍 성능과 안정성 모두 높지만, 90~100만 원대 가격이 부담이다.엔비디아 GTX 1080에 레이 트레이싱을 더한 성능, RTX 2070 슈퍼RTX 2070 슈퍼 관련 자료 (출처=IT동아)그렇다보니 중간에 위치한 RTX 2070 슈퍼가 주목받고 있다. RTX 2070 슈퍼는 이전 세대인 GTX 1080에 레이 트레이싱을 추가한 성능을 보인다. 평균 가격이 60만 원대라 150만 원대 게이밍 데스크톱을 맞출 때 1순위로 추천된다. 또 레이 트레이싱을 지원하는 제품 중에서 중간이니 1~2년 지나도 충분한 성능을 낸다.그래서 RTX 2070 슈퍼 파운더스 에디션과 동일한 성능으로 출시된 조텍 게이밍 지포스 RTX 2070 슈퍼 D6 8GB 트윈(이하 조텍 RTX 2070 슈퍼 8GB)을 통해 RTX 2070 슈퍼의 성능을 알아본다.조텍 게이밍 지포스 RTX 2070 슈퍼 D6 8GB 트윈으로 알아보는 RTX 2070 슈퍼조텍 게이밍 지포스 RTX 2070 슈퍼 D6 8GB (출처=IT동아)조텍 RTX 2070 슈퍼 8GB은 2560개 쿠다 코어와 8GB GDDR6 메모리를 갖춘 그래픽 카드다. 동작 속도는 엔비디아가 표준으로 제시한 파운더스 에디션과 동일한 베이스 클럭 1,605MHz, 부스트 클럭 1,770MHz다. 파운더스 에디션(레퍼런스 카드)과 차이점이 있다면, 전원부 부품과 쿨링 방열판, 쿨러같은 하드웨어를 개선해, 더욱 안정적인 온도와 저소음으로 동작한다.조텍 RTX 2070 슈퍼 8GB의 외부 입력 인터페이스 (출처=IT동아)외부 입력 인터페이스는 최대 4K(4,096x2,160) 60Hz 주사율을 지원하는 3개의 디스플레이포트 1.4, 최대 4K(3,840x2,160) 60Hz 주사율을 지원하는 HDMI 2.0 포트 1개가 적용돼있다. 디스플레이포트 이용 시 16:10 비율의 4K 모니터까지 지원하며, HDMI 이용 시 16:9 비율의 4K 모니터를 사용할 수 있다. 주사율은 모니터가 초당 출력할 수 있는 재생 횟수로, 60Hz는 초당 60회 화면을 갱신한다는 의미다.그래픽 카드 성능을 수치화시켜주는 벤치마크 프로그램. (출처=IT동아)게이밍 그래픽 카드 성능을 수치화하는 프로그램, 3D 마크:파이어 스트라이크 결과를 보자. 상단 결과는 FHD 기반 테스트 결과며, 하단 결과는 4K 테스트 결과다. 종합 점수는 CPU 결과가 포함된 점수니, 좌측 하단에 기재된 'Graphics Score' 점수만 보면 된다.조텍 RTX 2070 슈퍼 8GB는 FHD 테스트인 파이어 스트라이크에서 24,954점을 획득했고, 두 개의 테스트 구간에서 각각 97프레임, 122프레임을 발휘했다. FHD 기반 게임이라면 평균 110프레임 정도 낸다.4K 테스트 결과인 파이어 스트라이트 울트라에서는 6,020점을 획득했고, 두 개의 테스트 구간에서 각각 33프레임과 21프레임을 획득했다. 4K 게임을 최상급 옵션으로 즐긴다면30프레임 정도 낸다는 뜻인데, 옵션을 조금만 낮춘다면 4K 60프레임 게이밍도 원활한 수준이다.데이어스 엑스 : 맨카인드 디바이디드 벤치마크 결과 (출처=IT동아)게임에서는 어떨까? 데이어스 엑스 : 맨카인드 디바이디드 벤치마크를 각각 4K, FHD로 진행했다. 출시한 지 3년정도 지났으나, 권장 사양으로 엔비디아 GTX 970에 16GB 메모리를 요구할 만큼 사양이 높다. 또 최근 게이밍 모니터는 144Hz~240Hz 주사율을 지원하는데, 이를 원활히 활용하기 위해 권장 사양보다 높은 성능이 필요하다조텍 RTX 2070 슈퍼 8GB는 4K 해상도에서 평균 48.3 프레임, 최대 59.1프레임을 보였으며, FHD 해상도로는 평균 91.7프레임, 최대 142.9 프레임을 보여주었다. 출시 2~3년 지난 게임이라면 4K 60프레임 플레이도 충분하며, 출시 1년 이내 고사양 게임이라면 게이밍 옵션을 조금 낮출 필요가 있다.FHD는 최소 90~100프레임으로 확인된다. 배틀그라운드나 더 디비전처럼 144프레임이 유리한 1인칭 슈팅 게임이라면 최소 100프레임이 보장되며, 조금만 옵션을 낮추면 140~160프레임도 가능하다.평균 가격대가 60만 원대라 150만 원대 견적에 적합하다. (출처=IT동아)파스칼 아키텍처 기반 GTX 시리즈와 튜링 아키텍처 기반 GTX / RTX 그래픽 카드를 성능별로 나란히 놓는다면, RTX 2070 슈퍼의 위치는 중간에서 조금 더 위다. 이같은 입지는 상당히 중요한데, 게임사가 새 게임을 기획할 때, 가능한 많은 사람에게 판매하기 위해 평균 사양을 목표로 하거나, 아예 고성능을 요구한다. RTX 2070 슈퍼는 이 두 접점에 위치한 제품이라 장기간 활용에 유리하다.조텍 RTX 2070 슈퍼 8GB의 가격은 현재 50만 원대 후반이며, 오버클럭된 AMP 제품이 60만 원대 중반이다. 제조사가 다르더라도 RTX 2070 슈퍼의 가격대는 비슷한 수준이니150만 원대 게이밍 데스크톱을 생각하고 있거나, 60만 원대 그래픽 카드로 교체한다면 RTX 2070 슈퍼를 추천한다.동아닷컴 IT전문 남시현 기자 (shnam@donga.com)오늘의 핫이슈창닫기기사를 추천 하셨습니다[리뷰] RTX + 슈퍼(SUPER)는 무엇인가? 조텍 게이밍 지포스 RTX 2070 슈퍼 8GB 트윈베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\",\n",
       " \"제주국제공항에 설치된 ‘스마트 시큐리티’.㈜동곡기정은 창업 이후 현재까지 다수에 걸쳐 전국세관 및 공항 등에 X레이 검색기 및 보안장비를 설치하며 국가의 보안과 안전을 선도해 온 기업이다. 이 회사의 보안검색 장비는 전국 공항과 기밀 유지가 최우선시 되는 대기업을 비롯한 중견기업의 국내외 제조공장에서도 중요한 보안기능을 수행하고 있다. 이러한 보안검색 분야의 사업수행에 대한 공로를 인정받아 수차례 관세청장 표창장도 수상한 경력이 있다.최근에는 인공지능을 탑재한 각종 첨단 보안검색 장비를 개발 보급하며 국내 유관기관 및 대기업, 중견기업에서 보안 검색률을 크게 증대시킬 것이라는 평가를 받고 있다. 앞으로 회사는 빅데이터와 딥러닝 기술 개발을 통해 신속하고 정확한 보안을 구축할 수 있는 차세대 장비로 경쟁력을 키워나갈 계획이다.동곡기정 이광선 회장은 6·25 참전 유공자이며 호국영웅기장을 받은 인물로 국가가 위험했을 당시를 몸소 경험했다. 그가 가지고 있는 국가 안전에 대한 의지는 남다를 수밖에 없다.window.googletag = window.googletag || {cmd: []};\\n  googletag.cmd.push(function() {\\n    googletag.defineSlot('/1249652/Donga_OSV_Desktop_1x1', [1, 1], 'div-gpt-ad-1571163856651-0').addService(googletag.pubads());\\n    googletag.pubads().enableSingleRequest();\\n    googletag.pubads().collapseEmptyDivs();\\n    googletag.enableServices();\\n  });googletag.cmd.push(function() { googletag.display('div-gpt-ad-1571163856651-0'); });이 회장은 “철저한 검색과 보안은 국내 테러의 수많은 위험 요소를 줄일 수 있는 시발점”이라며 “보안검색 장비를 유통하는 업체로서 국가 안전과 항공 안전에 이바지해야 한다는 사명감을 가지고 있다”고 말했다. 사명감으로 50년간 항공보안업의 길을 걸어온 이 회장은 최근 한서대로부터 항공보안 산업 발전에 기여한 공로로 명예박사학위를 받았다. 그는 지금까지도 보안장비 관련 전시회를 다니며 기술력에 대해 끊임없이 연구하고 있다.주요기사명동-홍대앞 텅 빈 점포 “권리금 없습니다”…전통상권까지 불황 한파백원우팀원, 숨지기전 靑관계자와 5차례 통화그는 “보안검색 장비 분야에 대한 정부 차원의 지원이 필요한 때”라며 “연구개발을 이어나가고자 하는 기업에 재투자를 위한 자금지원과 보호정책에 대한 논의가 이뤄지길 바란다”고 당부했다.황효진 기자 herald99@donga.com창닫기기사를 추천 하셨습니다X레이검색기-보안장비로 국가 보안-안전 선도베스트 추천 뉴스“증오 조장하는 강제개종… 민주주의 국가 중 한국이 유일”[단독]檢, 윤건영-김경수 불러 감찰무마 의혹 조사청와대의 하명 수사 해명이 꼬이는 이유[청와대 풍향계/문병기][단독]檢, 송병기 피의자로 조사… 차명폰-외장하드 분석뒤 재조사 계획[단독]백원우팀원, 숨지기전 열흘간 靑민정실 관계자 한 명과 5차례 통화한국당 신임 원내대표에 5선 심재철…“총선 필승할 것”Copyright by dongA.com All rights reserved.$( document ).ready( function() {\\r\\n            window.bestPopupObj = Popup( $( '#bestnews_layer' ), {\\r\\n                    vertical: 'center',  /* top, bottom, center */\\r\\n                    horizontal: 'center',  /* left, right, center */\\r\\n                    //effect: 'slide',  /* clip slide blind */\\r\\n                    //direction: 'left',   /* up, down, left, right */\\r\\n                    duration: 300,\\r\\n                    //scroll: true\\r\\n                } ) ;\\r\\n          });\"]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "def donga_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "\n",
    "    \n",
    "    for i in range(1,50,15):\n",
    "        list_url= \"http://www.donga.com/news/search?p=\"+str(i)+\"&query=%EB%94%A5%EB%9F%AC%EB%8B%9D&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "\n",
    "        url = urllib.request.Request(list_url) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "       \n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "        \n",
    "        result = soup.find_all('p', class_=\"txt\")\n",
    "        \n",
    "        params = []\n",
    "        for i in result: # h2태그에서 \"headline mg\" 클래스를 가져옴.\n",
    "            for i2 in i: # h2 headline mg에서 a태그로 접근\n",
    "                params.append(i2.get(\"href\")) # 중첩포문으로 h2태그에서 a태그만 가져옴\n",
    "    return params\n",
    "\n",
    "print(donga_scroll())\n",
    "\n",
    "def donga_detail_scroll():\n",
    "    import  urllib.request # 웹에 있는 html 문서를 읽어오기 위해서 필요한 패키지\n",
    "    from  bs4  import  BeautifulSoup # html 문서에서 특정 데이터만 자유롭게 추출하기 위한 패키지\n",
    "    \n",
    "    params = []\n",
    "    list_url= donga_scroll() # 재귀함수로 불러오기!\n",
    "    f = open(\"d:\\\\data\\\\donga_deep_learning.txt\", \"w\", encoding = \"UTF-8\")\n",
    "    \n",
    "    for i in list_url:\n",
    "        url = urllib.request.Request(i) # 위의 url 정보를 파이썬에서 인식할 수 있도록 파싱한다.\n",
    "        # print (url) -> 메모리 주소가 결과로 출력된다.\n",
    "        result = urllib.request.urlopen(url).read().decode(\"utf-8\") # 한글이 포함되어져있으므로 .decode(\"utf-8\")을 써준다.\n",
    "        soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "        result = soup.find_all('div', class_=\"article_txt\") \n",
    "        for i in result: \n",
    "            params.append(i.get_text(\"\", strip = True))\n",
    "            \n",
    "    f.write(str(params)) \n",
    "               \n",
    "    f.close()\n",
    "    return params\n",
    "     \n",
    "\n",
    "    \n",
    "donga_detail_scroll()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
