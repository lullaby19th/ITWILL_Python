{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 예제150. 웹스크롤링한 텍스트 데이터를 워드 클라우드로 시각화 하기\n",
    "\n",
    "1. 아나콘다 프롬프트 창을 열고 wordcloud 패키지 설치\n",
    "\n",
    "conda install wordcloud\n",
    "\n",
    "or\n",
    "\n",
    "pip install wordcloud\n",
    "\n",
    "\n",
    "2. c드라이브 밑에 project 폴더를 생성\n",
    "\n",
    "\n",
    "3. project 폴더 밑에 3가지 파일을 둡니다\n",
    "\n",
    "    - usa_im.png\n",
    "    - s_korea.png  \n",
    "    - word.txt\n",
    "    - 어제 스크롤링했던 기사 파일(mytext15.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 11 09:57:22 2019\n",
    "\n",
    "@author: wdp\n",
    "\"\"\"\n",
    "\n",
    "# 텍스트마이닝 데이터 정제\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt # 그래프 그리는 모듈\n",
    "from os import path # os에 있는 파일을 파이썬에서 인식하기 위해서 필요\n",
    "import re # 데이터 정제에 필요한 모듈\n",
    "import numpy as np \n",
    "from PIL import Image #이미지 시각화 모듈\n",
    "\n",
    "# 워드 클라우드의 배경이 되는 이미지 모양\n",
    "usa_mask = np.array(Image.open(\"c:/project/usa_im.png\"))\n",
    "\n",
    "# 워드 클라우드를 그릴 스크립트의 이름을 물어봅니다.\n",
    "script = input( 'input file name : ')\n",
    "\n",
    "#워드 클라우드 그림이 저장될 작업 디렉토리를 설정\n",
    "d = path.dirname(\"c:/project/\")\n",
    "\n",
    "# 기사 스크립트와 os의 위치를 연결하여 utf-8로 인코딩해서 한글텍스트를\n",
    "# text 변수로 리턴한다. \n",
    "text = open(path.join(d, \"%s\"%script), mode=\"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# 파이썬이 인식할 수 있는 한글 단어의 갯수를 늘리기 위한 작업\n",
    "file = open('c:/project/word.txt', 'r', encoding = 'utf-8')\n",
    "word = file.read().split(' ')\n",
    "for i in word:\n",
    "    text = re.sub(i,'',text)\n",
    "\n",
    "# 워드 클라우드를 그린다.\n",
    "wordcloud = WordCloud(font_path='C://Windows//Fonts//gulim', # 글씨체\n",
    "                      stopwords=STOPWORDS, # 마침표, 느낌표, 싱글 쿼테이션 등을 정제(지움)\n",
    "                      max_words=1000, #워드 클라우드에 그릴 최대 단어갯수\n",
    "                      background_color='white', #배경 색깔\n",
    "                      max_font_size = 100, # 최대 글씨 크기(사용빈도에 따라 크기 결정)\n",
    "                      min_font_size = 1, # 최소 글씨 크기(사용빈도에 따라 크기 결정)\n",
    "                      mask = usa_mask, # 배경 모양\n",
    "                      colormap='jet').generate(text).to_file('c:/project/cnn_cloud.png')\n",
    "                      # c드라이브 밑에 project 폴더 밑에 생성되는 워드 클라우드 이미지 이름\n",
    "plt.figure(figsize=(15,15))  # 그림 사이즈\n",
    "plt.imshow(wordcloud, interpolation='bilinear') # interpolation = 글씨가 퍼지는 스타일 \n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "text.close()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제270.   \n",
    "## 어제 받은 기사를 우리나라 지도를 배경으로 워드 클라우드를 그리고 반 카톡으로 올리시오~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 11 09:57:22 2019\n",
    "\n",
    "@author: wdp\n",
    "\"\"\"\n",
    "\n",
    "# 텍스트마이닝 데이터 정제\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt # 그래프 그리는 모듈\n",
    "from os import path # os에 있는 파일을 파이썬에서 인식하기 위해서 필요\n",
    "import re # 데이터 정제에 필요한 모듈\n",
    "import numpy as np \n",
    "from PIL import Image #이미지 시각화 모듈\n",
    "\n",
    "# 워드 클라우드의 배경이 되는 이미지 모양\n",
    "usa_mask = np.array(Image.open(\"c:/project/s_korea.png\"))\n",
    "\n",
    "# 워드 클라우드를 그릴 스크립트의 이름을 물어봅니다.\n",
    "script = input( 'input file name : ')\n",
    "\n",
    "#워드 클라우드 그림이 저장될 작업 디렉토리를 설정\n",
    "d = path.dirname(\"c:/project/\")\n",
    "\n",
    "# 기사 스크립트와 os의 위치를 연결하여 utf-8로 인코딩해서 한글텍스트를\n",
    "# text 변수로 리턴한다. \n",
    "text = open(path.join(d, \"%s\"%script), mode=\"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# 파이썬이 인식할 수 있는 한글 단어의 갯수를 늘리기 위한 작업\n",
    "file = open('c:/project/word.txt', 'r', encoding = 'utf-8')\n",
    "word = file.read().split(' ')\n",
    "for i in word:\n",
    "    text = re.sub(i,'',text)\n",
    "\n",
    "# 워드 클라우드를 그린다.\n",
    "wordcloud = WordCloud(font_path='C://Windows//Fonts//gulim', # 글씨체\n",
    "                      stopwords=STOPWORDS, # 마침표, 느낌표, 싱글 쿼테이션 등을 정제(지움)\n",
    "                      max_words=1000, #워드 클라우드에 그릴 최대 단어갯수\n",
    "                      background_color='white', #배경 색깔\n",
    "                      max_font_size = 100, # 최대 글씨 크기(사용빈도에 따라 크기 결정)\n",
    "                      min_font_size = 1, # 최소 글씨 크기(사용빈도에 따라 크기 결정)\n",
    "                      mask = usa_mask, # 배경 모양\n",
    "                      colormap='jet').generate(text).to_file('c:/project/cnn2_cloud.png')\n",
    "                      # c드라이브 밑에 project 폴더 밑에 생성되는 워드 클라우드 이미지 이름\n",
    "    \n",
    "plt.figure(figsize=(15,15))  # 그림 사이즈\n",
    "plt.imshow(wordcloud, interpolation='bilinear') # interpolation = 글씨가 퍼지는 스타일 \n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "text.close()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제151. 이미지 데이터 스크롤링 하는 방법\n",
    "\n",
    "1. google에서 이미지를 스크롤링 하기\n",
    "\n",
    "    - 크롬 웹브라우져가 설치 되어져 있어야 한다.\n",
    "    - d 드라이브 밑에 chromedriver 폴더를 생성하고 거기에 chromedriver.exe를 넣어야 한다.\n",
    "    \n",
    "\n",
    "2. d 드라이브 밑에 googleimages 폴더를 생성하시오.\n",
    "\n",
    "3. 다운 받을 이미지를 햄버거 말고 유니크한걸로 생각하세요~\n",
    "\n",
    "4. 콘다 프롬프트 창을 열고 selenium을 설치하세요~(밑에 둘중 하나)\n",
    "    - pip install selenium\n",
    "    - conda install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "import urllib.request # 웹 url을 파이썬이 인식 할 수 있게하는 패키지\n",
    "from  bs4 import BeautifulSoup # html에서 데이터 검색을 용이하게 하는 패키지\n",
    "from selenium import webdriver  \n",
    "# selenium : 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 \n",
    "# 손으로 클릭하면서 컴퓨터가 대신하면서 스크롤링하게 하는 패키지\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time       # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 아래 파일경로의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe'\n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# 구글의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url) \n",
    "browser.get(\"https://www.google.co.kr/imghp?hl=ko&tab=wi&ei=l1AdWbegOcra8QXvtr-4Cw&ved=0EKouCBUoAQ\") \n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "# elem = browser.find_elements_by_class_name('gLFyf gsfi') # Tip : f12누른후 커서를 검색창에 올려두고 id를 찾으면 best\n",
    "\n",
    "elem = browser.find_element_by_xpath(\"//*[@class='gLFyf gsfi']\")  # 위의 코드대로 하거나 이렇게 하거나 둘 중 하나 select\n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"윤하 winter flower\") # 여기에 스크롤링하고싶은 검색어를 입력\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "# 현재 결과 더보기는 구현 되어있지 않은상태 -> 구글의 경우 400개 image가 저장됨.\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 5): # 4번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "browser.find_element_by_xpath(\"//*[@class='ksb']\").send_keys(Keys.ENTER)  # 여기가 결과 더보기 코드입니다\n",
    "\n",
    "for i in range(1,5):\n",
    "\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "\n",
    "    time.sleep(10) \n",
    "\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "### 검색한 구글 이미지의 url을 따오는 코드 ###\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_=\"rg_ic rg_i\")  # 구글 이미지 url 이 있는 img 태그의 _img 클래스에 가서 (f12로 확인가능.)\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음.\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# except부분\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~  \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"D:/googleImages/\" + str(idx) + \".jpg\")\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제271.  \n",
    "## 다운 받은 이미지를 신경망에 넣기 전에 사이즈를 일괄 조정해야 하므로  \n",
    "## 32x32 사이즈로 조정하시오~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제272. (또 다른 점심시간 문제)\n",
    "\n",
    "## bing에서 이미지 검색하는 웹 스크롤링 코드를 작성하시오~  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "import urllib.request # 웹 url을 파이썬이 인식 할 수 있게하는 패키지\n",
    "from  bs4 import BeautifulSoup # html에서 데이터 검색을 용이하게 하는 패키지\n",
    "from selenium import webdriver  \n",
    "# selenium : 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 \n",
    "# 손으로 클릭하면서 컴퓨터가 대신하면서 스크롤링하게 하는 패키지\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time       # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 아래 파일경로의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe' \n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# Bing의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url) \n",
    "browser.get(\"https://www.bing.com/images?FORM=Z9LH\")  # bing의 이미지 검색창에 들어가서 url을 가져오면 됨.\n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "# elem = browser.find_elements_by_class_name('b_searchbox') # Tip : f12누른후 커서를 검색창에 올려두고 class이름이나 id 찾아보기.\n",
    "\n",
    "elem = browser.find_element_by_xpath(\"//*[@class='b_searchbox']\")  # 위의 코드대로 하거나 이렇게 하거나 둘 중 하나 select\n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"비긴어게인 독일\") # 여기에 스크롤링하고싶은 검색어를 입력\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "# 현재 결과 더보기는 구현 되어있지 않은상태 -> 구글의 경우 400개 image가 저장됨.\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 5): # 4번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "### 검색한 bing 이미지의 url을 따오는 코드 ###\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_= \"mimg\")  # bing 이미지 url 이 있는 img 태그의 _img 클래스에 가서 (f12로 확인가능.)\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음.\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# except부분\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~  \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"D:/bingImages/\" + str(idx) + \".jpg\")  # 저장 폴더 위치 바꾸기\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제152. 네이버 이미지 스크롤링\n",
    "\n",
    "### 네이버 이미지 검색 URL\n",
    "\n",
    "https://search.naver.com/search.naver?where=image&sm=tab_jum&query="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "import urllib.request # 웹 url을 파이썬이 인식 할 수 있게하는 패키지\n",
    "from  bs4 import BeautifulSoup # html에서 데이터 검색을 용이하게 하는 패키지\n",
    "from selenium import webdriver  \n",
    "# selenium : 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 \n",
    "# 손으로 클릭하면서 컴퓨터가 대신하면서 스크롤링하게 하는 패키지\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time       # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 아래 파일경로의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe' \n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# naver의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url) \n",
    "browser.get(\"https://search.naver.com/search.naver?where=image&amp;sm=stb_nmr&amp;\")  \n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "elem = browser.find_element_by_id(\"nx_query\") # Tip : f12누른후 커서를 검색창에 올려두고 class이름이나 id 찾아보기.\n",
    "                                               # ID를 입력할때는 이렇게 by_class_name 을 지우고 by_id입력.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"레드벨벳 슬기\") # 여기에 스크롤링하고싶은 검색어를 입력\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "# 현재 결과 더보기는 구현 되어있지 않은상태 -> 구글의 경우 400개 image가 저장됨.\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 4): # 5번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "browser.find_element_by_xpath(\"//*[@class='more_img']\").send_keys(Keys.ENTER)  # 여기가 결과 더보기 코드입니다\n",
    "\n",
    "for i in range(1,3):\n",
    "\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "\n",
    "    time.sleep(10) \n",
    "\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "### 검색한 네이버 이미지의 url을 따오는 코드 ###\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_= \"_img\")  # naver 이미지 url 이 있는 img 태그의 _img 클래스에 가서 (f12로 확인가능.)\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음.\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# except부분\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~  \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"D:/naver/\" + str(idx) + \".jpg\")  # 저장 폴더 위치 바꾸기\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다운받은 이미지 파일들의 사이즈를 조절하는 방법(resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 # 이미지나 영상을 처리하는 모듈\n",
    "\n",
    "path = [\"D:\\\\googleImages\"]\n",
    "j = 0\n",
    "for k in path:\n",
    "    file_list = os.listdir(k)\n",
    "    file_name = []\n",
    "    for i in file_list:\n",
    "        if  i not in ['resized_2.jpg','resized_3.jpg','resized_4.jpg']:\n",
    "            file_name.append(int( i[:-4] ))\n",
    "    file_name.sort()\n",
    "    file_list = [k + '\\\\' + str(i) + '.jpg' for i in file_name]\n",
    "    \n",
    "    for i in file_list:\n",
    "        img = cv2.imread(i)\n",
    "        width, height = img.shape[:2]\n",
    "        resize = cv2.resize(img, (int(32), int(32)), interpolation=cv2.INTER_CUBIC) # 현재 32 x 32로 resize\n",
    "        cv2.imwrite(\"d:\\\\resize\\\\\" + str(j + 1) + '.jpg', resize)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제153. 동영상 데이터를 웹스크롤링 하는 방법\n",
    "\n",
    "1. 유튜브에 점속한다.(www.youtube.com)\n",
    "\n",
    "2. 원하는 동영상의 playlist url을 검색해야한다.\n",
    "\n",
    "3. 카페에 유튜브 동영상 스크롤링 코드를 가져온다.\n",
    "\n",
    "4. 아나콘다 프롬프트 창을 열고\n",
    "\n",
    "    - pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 11 08:34:57 2019\n",
    "\n",
    "@author: wdp\n",
    "\"\"\"\n",
    "\n",
    "from pytube import YouTube # 유튜브 동영상 다운로드 모듈\n",
    "\n",
    "import requests # 웹 url을 파이썬이 인식할 수 있또록 하는 모듈\n",
    "\n",
    "import os # 동영상 다운로드할 os의 위치를 지정하기 위해\n",
    "\n",
    "import sys\n",
    "\n",
    "# 파이썬의 워킹 디렉토리인 C:\\Users\\wdp의 위치를 인식하기 위해서\n",
    "#workdir = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "\n",
    "workdir = os.path.dirname(os.path.abspath(sys.argv[0])) # workdir\n",
    "\n",
    "url = input('playlist url : ').rstrip()\n",
    "\n",
    "\n",
    "\n",
    "if 'watch' in url:\n",
    "\n",
    "    url = 'https://youtube.com/playlist?list=' + url.split('list=')[-1].split('&')[0]\n",
    "\n",
    "\n",
    "\n",
    "res = requests.get(url)\n",
    "\n",
    "source = res.text #Response의 바디를 source라는 변수에 저장합니다. 이는 Raw Text 입니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vid_ids = [x.split('href=\"/watch?v=')[1].split('&amp;')[0] for x in source.split('\\n') if 'pl-video-title-link' in x]\n",
    "\n",
    "\n",
    "\n",
    "for vid in vid_ids:\n",
    "\n",
    "    getStr = 'https://www.youtube.com/watch?v=' + vid\n",
    "\n",
    "    yt = YouTube(getStr)\n",
    "\n",
    "    file_name = yt.title\n",
    "\n",
    "    print('Downloading %s' % (file_name))\n",
    "\n",
    "\n",
    "\n",
    "    yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download('C:\\\\data') # 파일경로지정\n",
    "    #file_extension은 재생목록 파일들 중에서 mp4만 추출해서 다운로드 받겠다 는뜻\n",
    "    # progressive는 720p까지만 지원 가능한 다운로드 방식\n",
    "\n",
    "    output = ''\n",
    "\n",
    "\n",
    "    print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동영상 다운로드 받는 방식 2가지\n",
    "\n",
    "1. adaptive 방식의 코드 : 동영상과 음성을 분리해서 받아가지고 다시 합치는 코드\n",
    "\n",
    "2. progressive 방식의 코드 : 분리하지 않고 받는 코드 (현재코드)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제154. 서울시 응답소 게시판 웹스크롤링 하는 방법\n",
    "\n",
    "### 이때까지 해본 스크롤링 리스트\n",
    "1. 레이디 버그\n",
    "2. 중앙일보 신문사\n",
    "3. 동아일보 신문사\n",
    "4. 구글이미지\n",
    "5. 빙 이미지\n",
    "6. 네이버 이미지\n",
    "7. 유튜브 동영상\n",
    "8. 서울시 응답소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 11 16:23:06 2019\n",
    "\n",
    "@author: wdp\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request  # 웹브라우저에서 html 문서를 얻어오기위해 통신하는 모듈\n",
    "from  bs4 import BeautifulSoup  # html 문서 검색 모듈\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 웹스크롤링한 글을 생성하는 텍스트 파일을 만드는 함수\n",
    "# 입력한 디렉토리가 없으면 새로 생성합니다.\n",
    "# d:\\\\data\\\\aaaa.txt\n",
    "\n",
    "def get_save_path():\n",
    "    save_path = input(\"Enter the file name and file location :\" )\n",
    "    save_path = save_path.replace(\"\\\\\", \"/\")\n",
    "    print(save_path) #d:\\\\data\\\\aaa.txt\n",
    "    print(os.path.split(save_path)[0]) #d:\\\\data\n",
    "    if not os.path.isdir(os.path.split(save_path)[0]):\n",
    "        os.mkdir(os.path.split(save_path)[0])\n",
    "        \n",
    "    # if not ~ 입력한 경로가 존재하지 않으면 경로를 새로 만들겠다.\n",
    "    return save_path\n",
    "\n",
    "# 응답소 게시판 메인 페이지에서 아래의 페이지 번호 29페이지까지 \n",
    "# 실제 글 올린 게시판의 상세 url을 가져오는 함수\n",
    "\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    for j in range(1, 30):\n",
    "\n",
    "        list_url = \"http://eungdapso.seoul.go.kr/Shr/Shr01/Shr01_lis.jsp\"\n",
    "        \n",
    "        # for 문으로 page=1 ~ page=29까지의 문자를 생성\n",
    "     \n",
    "        request_header = urllib.parse.urlencode({\"page\": j})\n",
    "        \n",
    "        # print (request_header) # 결과 page=1, page=2 .. (java script 와 관련있음.)\n",
    "        # 위에서 생성한 page=1 ~ page=29까지 문자를 컴퓨터가 인식할 수 있도록\n",
    "        # encoding한다.\n",
    "        \n",
    "\n",
    "        request_header = request_header.encode(\"utf-8\")\n",
    "        # print (request_header) # b'page=29'\n",
    "        \n",
    "        # 응답소 게시판 url과 페이지 번호를 붙여서 url을 만든다.\n",
    "        \n",
    "        url = urllib.request.Request(list_url, request_header)\n",
    "        # print (url) # <urllib.request.Request object at 0x00000000021FA2E8>\n",
    "\n",
    "\n",
    "        # 위에서 생성한 url을 가지고 html코드를 뽑아낸다.\n",
    "        res = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "\n",
    "        # 위에서 생성한 html코드를 BeautifulSoa가 이해할 수 있도록 파싱.\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        \n",
    "        # li 테그의 클래스 pclist_list_tit2의 html 문서를 리스트화 한다.\n",
    "        soup2 = soup.find_all(\"li\", class_=\"pclist_list_tit42\")\n",
    "        \n",
    "        # 리스트에서 html 코드를 하나씩 뽑아내면서 상세 url을 가져온다\n",
    "        for soup3 in soup2:\n",
    "            soup4 = soup3.find(\"a\")[\"href\"]\n",
    "            params.append(re.search(\"[0-9]{14}\", soup4).group())\n",
    "            \n",
    "            # 숫자부분만 가져온다. [0-9]{14} 0에서 9까지 14자리\n",
    "            \n",
    "    return params\n",
    "\n",
    "print(fetch_list_url())\n",
    "\n",
    "#'20190326900065', 이런식의 숫자가 여러개나옴\n",
    "\n",
    "def fetch_list_url2():\n",
    "\n",
    "    params2 = fetch_list_url()\n",
    "\n",
    "    f = open(get_save_path(), 'w', encoding =\"utf-8\")\n",
    "\n",
    "    for i in params2:\n",
    "\n",
    "        detail_url = \"http://eungdapso.seoul.go.kr/Shr/Shr01/Shr01_vie.jsp\"\n",
    "\n",
    "        request_header = urllib.parse.urlencode({\"RCEPT_NO\": str(i) })\n",
    "        request_header = request_header.encode(\"utf-8\")\n",
    "\n",
    "        url = urllib.request.Request(detail_url, request_header)\n",
    "        res = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        soup2 = soup.find(\"div\", class_=\"form_table\")\n",
    "\n",
    "        tables = soup2.find_all(\"table\")\n",
    "        table0   = tables[0].find_all(\"td\")\n",
    "        table1   = tables[1].find(\"div\",class_=\"table_inner_desc\")\n",
    "        table2   = tables[2].find(\"div\",class_=\"table_inner_desc\")\n",
    "\n",
    "        date  = table0[1].get_text()\n",
    "        title = table0[0].get_text()\n",
    "        question = table1.get_text(strip=True)\n",
    "        answer   = table2.get_text(strip=True)\n",
    "\n",
    "        f.write(\"==\" * 30 + \"\\n\")\n",
    "        f.write(title + \"\\n\")\n",
    "        f.write(date + \"\\n\")\n",
    "        f.write(question + \"\\n\")\n",
    "        f.write(answer + \"\\n\")\n",
    "        f.write(\"==\" * 30 + \"\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "fetch_list_url2()\n",
    "\n",
    "# 인원수가 한꺼번에 들어갈 경우\n",
    "# <urlopen error [WinError 10060] 연결된 구성원으로부터 응답이 없어 연결하지 못했거나, 호스트로부터 응답이 없어 연결이 끊어졌습니다>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
